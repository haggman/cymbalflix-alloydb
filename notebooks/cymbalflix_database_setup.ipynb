{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé¨ CymbalFlix Discover - Database Setup\n",
    "\n",
    "Welcome to the data engineering portion of CymbalFlix Discover! In this notebook, you'll set up your AlloyDB database with everything needed for an AI-powered movie discovery application.\n",
    "\n",
    "## What We're Building\n",
    "\n",
    "By the end of this notebook, your database will contain:\n",
    "\n",
    "| Table | Records | Purpose |\n",
    "|-------|---------|--------|\n",
    "| `movies` | ~9,700 | Core catalog with AI-searchable summaries and vector embeddings |\n",
    "| `genres` | 20 | Genre lookup table |\n",
    "| `movie_genres` | ~21,000 | Many-to-many junction for movie genres |\n",
    "| `users` | 610 | User profiles extracted from ratings data |\n",
    "| `ratings` | 100,836 | Historical ratings for analytics |\n",
    "| `tags` | 3,683 | User-generated tags for semantic analysis |\n",
    "| `watchlist` | 0 | Ready for user watchlist operations |\n",
    "\n",
    "## AlloyDB Extensions We'll Enable\n",
    "\n",
    "- **`vector`** - PostgreSQL vector data type for embeddings\n",
    "- **`alloydb_scann`** - Google's ScaNN index for lightning-fast vector search\n",
    "- **`google_ml_integration`** - Direct Vertex AI access from SQL\n",
    "\n",
    "## Security: IAM Authentication\n",
    "\n",
    "Notice something missing? **No database passwords!** We're using IAM authentication, which means:\n",
    "- Your Google Cloud identity is your database identity\n",
    "- No passwords to manage, rotate, or accidentally commit to Git\n",
    "- The Auth Proxy handles secure token exchange automatically\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Configure Your Environment\n",
    "\n",
    "First, let's set up the configuration for your specific AlloyDB cluster. You'll need your project ID from the lab instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Update these values with your environment details\n",
    "# =============================================================================\n",
    "\n",
    "# Your Google Cloud Project ID (from the lab instructions)\n",
    "PROJECT_ID = \"YOUR_PROJECT_ID\"  # TODO: Replace with your project ID\n",
    "\n",
    "# AlloyDB cluster details (from Terraform outputs)\n",
    "REGION = \"us-central1\"\n",
    "CLUSTER_ID = \"cymbalflix-cluster\"\n",
    "\n",
    "# The database we'll create for CymbalFlix\n",
    "DB_NAME = \"cymbalflix\"\n",
    "\n",
    "# GCS bucket with our MovieLens data\n",
    "DATA_BUCKET = \"gs://class-demo/ml-latest-small\"\n",
    "\n",
    "print(f\"‚úÖ Configuration set for project: {PROJECT_ID}\")\n",
    "print(f\"‚úÖ AlloyDB cluster: {CLUSTER_ID} in {REGION}\")\n",
    "print(f\"\\nüîê Using IAM authentication (no password required!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Get Your IAM Identity\n",
    "\n",
    "With IAM authentication, your Google Cloud identity becomes your database identity. Let's see who you are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "\n",
    "# Get the current authenticated user's email\n",
    "result = subprocess.run(\n",
    "    [\"gcloud\", \"auth\", \"list\", \"--filter=status:ACTIVE\", \"--format=json\"],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "accounts = json.loads(result.stdout)\n",
    "\n",
    "if accounts:\n",
    "    DB_USER = accounts[0]['account']\n",
    "    print(f\"‚úÖ Authenticated as: {DB_USER}\")\n",
    "    print(f\"\\n   This identity will be used for database connections.\")\n",
    "    print(f\"   No password needed - IAM handles authentication!\")\n",
    "else:\n",
    "    print(\"‚ùå No active gcloud account found. Please authenticate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Install and Start the AlloyDB Auth Proxy\n",
    "\n",
    "The AlloyDB Auth Proxy provides secure, IAM-based authentication to your database. With the `--auto-iam-authn` flag, it automatically:\n",
    "\n",
    "1. Gets your current Google Cloud identity\n",
    "2. Generates a short-lived access token\n",
    "3. Authenticates to AlloyDB on your behalf\n",
    "\n",
    "**Why Auth Proxy?**\n",
    "- No passwords to manage\n",
    "- Automatic token rotation\n",
    "- Encrypted connections\n",
    "- Works the same in development and production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and install the AlloyDB Auth Proxy\n",
    "!curl -o alloydb-auth-proxy https://storage.googleapis.com/alloydb-auth-proxy/v1.11.2/alloydb-auth-proxy.linux.amd64\n",
    "!chmod +x alloydb-auth-proxy\n",
    "\n",
    "print(\"\\n‚úÖ AlloyDB Auth Proxy installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Build the instance connection name\n",
    "INSTANCE_CONNECTION = f\"projects/{PROJECT_ID}/locations/{REGION}/clusters/{CLUSTER_ID}/instances/cymbalflix-primary\"\n",
    "\n",
    "print(f\"üîó Connecting to: {INSTANCE_CONNECTION}\")\n",
    "print(f\"üîê Using IAM authentication for: {DB_USER}\")\n",
    "print(\"\\n‚è≥ Starting Auth Proxy in background...\")\n",
    "\n",
    "# Start the proxy with IAM authentication enabled\n",
    "proxy_process = subprocess.Popen(\n",
    "    [\n",
    "        \"./alloydb-auth-proxy\", \n",
    "        INSTANCE_CONNECTION, \n",
    "        \"--port=5432\", \n",
    "        \"--address=127.0.0.1\",\n",
    "        \"--auto-iam-authn\"  # This enables automatic IAM authentication!\n",
    "    ],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE\n",
    ")\n",
    "\n",
    "# Give it a moment to start up\n",
    "time.sleep(5)\n",
    "\n",
    "# Check if it's running\n",
    "if proxy_process.poll() is None:\n",
    "    print(\"‚úÖ Auth Proxy is running with IAM authentication!\")\n",
    "    print(\"   Connection available at: localhost:5432\")\n",
    "    print(f\"   Authenticating as: {DB_USER}\")\n",
    "else:\n",
    "    print(\"‚ùå Auth Proxy failed to start. Check the error output below:\")\n",
    "    stdout, stderr = proxy_process.communicate()\n",
    "    print(stderr.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Install Python Dependencies and Connect\n",
    "\n",
    "We'll use `psycopg2` for PostgreSQL connectivity and `pandas` for data manipulation. These are the same tools you'd use with any PostgreSQL database‚ÄîAlloyDB is 100% PostgreSQL compatible!\n",
    "\n",
    "**Note on IAM auth:** When using the Auth Proxy with `--auto-iam-authn`, we don't need to provide a password. The proxy handles authentication automatically using your Google Cloud identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q psycopg2-binary pandas google-cloud-storage\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "\n",
    "def get_connection(dbname=\"postgres\"):\n",
    "    \"\"\"\n",
    "    Create a connection to AlloyDB via the Auth Proxy.\n",
    "    \n",
    "    With --auto-iam-authn enabled on the proxy, we don't need a password!\n",
    "    The proxy automatically generates and uses IAM-based credentials.\n",
    "    \"\"\"\n",
    "    return psycopg2.connect(\n",
    "        host=\"127.0.0.1\",\n",
    "        port=5432,\n",
    "        user=DB_USER,\n",
    "        dbname=dbname,\n",
    "        # No password needed! Auth Proxy handles IAM authentication.\n",
    "        # This is more secure than managing database passwords.\n",
    "    )\n",
    "\n",
    "# Test the connection\n",
    "try:\n",
    "    conn = get_connection()\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"SELECT version();\")\n",
    "        version = cur.fetchone()[0]\n",
    "        cur.execute(\"SELECT current_user;\")\n",
    "        current_user = cur.fetchone()[0]\n",
    "    conn.close()\n",
    "    print(\"‚úÖ Successfully connected to AlloyDB!\")\n",
    "    print(f\"\\nüîê Connected as: {current_user}\")\n",
    "    print(f\"\\nüìä Database version:\\n{version}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n",
    "    print(\"\\nüîç Troubleshooting tips:\")\n",
    "    print(\"   1. Make sure your AlloyDB cluster is fully created (check Cloud Console)\")\n",
    "    print(\"   2. Verify the Auth Proxy is running (re-run the previous cell)\")\n",
    "    print(\"   3. Check that PROJECT_ID and CLUSTER_ID are correct\")\n",
    "    print(\"   4. Ensure your IAM user was created by Terraform (check terraform output)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Create the CymbalFlix Database and Enable Extensions\n",
    "\n",
    "Now we'll create our dedicated database and enable the AlloyDB extensions that power our AI features:\n",
    "\n",
    "- **`vector`**: Adds the VECTOR data type for storing embeddings\n",
    "- **`alloydb_scann`**: Enables ScaNN indexes for fast similarity search\n",
    "- **`google_ml_integration`**: Connects AlloyDB directly to Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the cymbalflix database\n",
    "conn = get_connection(\"postgres\")\n",
    "conn.autocommit = True  # Required for CREATE DATABASE\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    # Check if database exists\n",
    "    cur.execute(\"SELECT 1 FROM pg_database WHERE datname = %s\", (DB_NAME,))\n",
    "    exists = cur.fetchone()\n",
    "    \n",
    "    if not exists:\n",
    "        cur.execute(f\"CREATE DATABASE {DB_NAME}\")\n",
    "        print(f\"‚úÖ Created database: {DB_NAME}\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è  Database {DB_NAME} already exists\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable AlloyDB extensions\n",
    "conn = get_connection(DB_NAME)\n",
    "conn.autocommit = True\n",
    "\n",
    "extensions = [\n",
    "    (\"vector\", \"Vector data type for embeddings\"),\n",
    "    (\"alloydb_scann\", \"ScaNN index for vector similarity search\"),\n",
    "    (\"google_ml_integration\", \"Vertex AI integration for AI SQL functions\")\n",
    "]\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    for ext_name, description in extensions:\n",
    "        try:\n",
    "            cur.execute(f\"CREATE EXTENSION IF NOT EXISTS {ext_name}\")\n",
    "            print(f\"‚úÖ Enabled: {ext_name}\")\n",
    "            print(f\"   ‚îî‚îÄ {description}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not enable {ext_name}: {e}\")\n",
    "\n",
    "conn.close()\n",
    "print(\"\\nüéâ AlloyDB is ready for AI-powered operations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Create the Database Schema\n",
    "\n",
    "Our schema is designed for both transactional operations (watchlists, ratings) and analytical queries (trending movies, genre analysis). The `movies` table includes a `summary_embedding` column that stores 1536-dimensional vectors for semantic search.\n",
    "\n",
    "**Schema Highlights:**\n",
    "- Normalized genre data with a junction table\n",
    "- Vector column for semantic similarity search\n",
    "- Proper foreign keys for data integrity\n",
    "- Timestamps for temporal analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our database schema\n",
    "schema_sql = \"\"\"\n",
    "-- Core movie catalog with vector embeddings for semantic search\n",
    "CREATE TABLE IF NOT EXISTS movies (\n",
    "    movie_id INTEGER PRIMARY KEY,\n",
    "    title VARCHAR(255) NOT NULL,\n",
    "    year INTEGER,\n",
    "    summary TEXT,\n",
    "    summary_embedding VECTOR(1536)\n",
    ");\n",
    "\n",
    "-- Genre lookup table\n",
    "CREATE TABLE IF NOT EXISTS genres (\n",
    "    genre_id SERIAL PRIMARY KEY,\n",
    "    genre_name VARCHAR(50) UNIQUE NOT NULL\n",
    ");\n",
    "\n",
    "-- Many-to-many junction table for movie genres\n",
    "CREATE TABLE IF NOT EXISTS movie_genres (\n",
    "    movie_id INTEGER REFERENCES movies(movie_id) ON DELETE CASCADE,\n",
    "    genre_id INTEGER REFERENCES genres(genre_id) ON DELETE CASCADE,\n",
    "    PRIMARY KEY (movie_id, genre_id)\n",
    ");\n",
    "\n",
    "-- User profiles (extracted from ratings data)\n",
    "CREATE TABLE IF NOT EXISTS users (\n",
    "    user_id INTEGER PRIMARY KEY,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\n",
    "-- Historical ratings for analytics\n",
    "CREATE TABLE IF NOT EXISTS ratings (\n",
    "    rating_id SERIAL PRIMARY KEY,\n",
    "    user_id INTEGER REFERENCES users(user_id) ON DELETE CASCADE,\n",
    "    movie_id INTEGER REFERENCES movies(movie_id) ON DELETE CASCADE,\n",
    "    rating NUMERIC(2,1) NOT NULL CHECK (rating >= 0.5 AND rating <= 5.0),\n",
    "    rated_at TIMESTAMP\n",
    ");\n",
    "\n",
    "-- User watchlists (for transactional operations)\n",
    "CREATE TABLE IF NOT EXISTS watchlist (\n",
    "    user_id INTEGER REFERENCES users(user_id) ON DELETE CASCADE,\n",
    "    movie_id INTEGER REFERENCES movies(movie_id) ON DELETE CASCADE,\n",
    "    added_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    PRIMARY KEY (user_id, movie_id)\n",
    ");\n",
    "\n",
    "-- User-generated tags for semantic analysis\n",
    "CREATE TABLE IF NOT EXISTS tags (\n",
    "    tag_id SERIAL PRIMARY KEY,\n",
    "    user_id INTEGER REFERENCES users(user_id) ON DELETE CASCADE,\n",
    "    movie_id INTEGER REFERENCES movies(movie_id) ON DELETE CASCADE,\n",
    "    tag_text VARCHAR(255) NOT NULL,\n",
    "    tagged_at TIMESTAMP\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Execute the schema\n",
    "conn = get_connection(DB_NAME)\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(schema_sql)\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"‚úÖ Database schema created!\")\n",
    "print(\"\\nüìã Tables created:\")\n",
    "print(\"   ‚Ä¢ movies (with vector embedding column)\")\n",
    "print(\"   ‚Ä¢ genres\")\n",
    "print(\"   ‚Ä¢ movie_genres (junction table)\")\n",
    "print(\"   ‚Ä¢ users\")\n",
    "print(\"   ‚Ä¢ ratings\")\n",
    "print(\"   ‚Ä¢ watchlist\")\n",
    "print(\"   ‚Ä¢ tags\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Load Data from Google Cloud Storage\n",
    "\n",
    "Now comes the fun part‚Äîloading our MovieLens data! We'll load data from GCS and transform it as we go:\n",
    "\n",
    "1. **Movies**: Extract year from title, e.g., \"Toy Story (1995)\" ‚Üí title=\"Toy Story\", year=1995\n",
    "2. **Summaries**: AI-generated movie descriptions (we'll merge these into movies)\n",
    "3. **Embeddings**: Pre-computed 1536-dimensional vectors from Gemini\n",
    "4. **Genres**: Parse pipe-delimited genres into a normalized structure\n",
    "5. **Users**: Extract unique user IDs from ratings\n",
    "6. **Ratings & Tags**: Load with timestamp conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_from_gcs(bucket_name, blob_name):\n",
    "    \"\"\"Load a CSV file from GCS into a pandas DataFrame.\"\"\"\n",
    "    # Parse the bucket path\n",
    "    if bucket_name.startswith(\"gs://\"):\n",
    "        bucket_name = bucket_name[5:]\n",
    "    \n",
    "    # Handle bucket/path format\n",
    "    if \"/\" in bucket_name:\n",
    "        parts = bucket_name.split(\"/\", 1)\n",
    "        bucket_name = parts[0]\n",
    "        blob_name = f\"{parts[1]}/{blob_name}\"\n",
    "    \n",
    "    client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    \n",
    "    content = blob.download_as_text()\n",
    "    return pd.read_csv(io.StringIO(content))\n",
    "\n",
    "print(\"‚úÖ GCS loader function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Load and Transform Movies\n",
    "\n",
    "The MovieLens dataset stores the year in the title (e.g., \"Jumanji (1995)\"). We'll extract it into a separate column for better analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load movies from GCS\n",
    "print(\"üì• Loading movies.csv from GCS...\")\n",
    "movies_df = load_csv_from_gcs(DATA_BUCKET, \"movies.csv\")\n",
    "print(f\"   Loaded {len(movies_df)} movies\")\n",
    "\n",
    "# Extract year from title using regex\n",
    "# Pattern matches \" (YYYY)\" at the end of the title\n",
    "def extract_year_and_clean_title(title):\n",
    "    match = re.search(r'\\s*\\((\\d{4})\\)\\s*$', str(title))\n",
    "    if match:\n",
    "        year = int(match.group(1))\n",
    "        clean_title = re.sub(r'\\s*\\(\\d{4}\\)\\s*$', '', title).strip()\n",
    "        return clean_title, year\n",
    "    return title, None\n",
    "\n",
    "# Apply the transformation\n",
    "movies_df[['clean_title', 'year']] = movies_df['title'].apply(\n",
    "    lambda x: pd.Series(extract_year_and_clean_title(x))\n",
    ")\n",
    "movies_df['title'] = movies_df['clean_title']\n",
    "movies_df = movies_df.drop(columns=['clean_title'])\n",
    "\n",
    "# Store genres for later processing\n",
    "movies_with_genres = movies_df[['movieId', 'genres']].copy()\n",
    "\n",
    "print(\"\\n‚úÖ Movies processed!\")\n",
    "print(f\"\\nüìä Sample data:\")\n",
    "movies_df[['movieId', 'title', 'year']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Load and Merge Summaries\n",
    "\n",
    "The summaries were generated using an AI model to provide rich, searchable descriptions of each movie. These descriptions enable semantic search‚Äîfinding movies based on meaning, not just keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load summaries\n",
    "print(\"üì• Loading summaries.csv from GCS...\")\n",
    "summaries_df = load_csv_from_gcs(DATA_BUCKET, \"summaries.csv\")\n",
    "print(f\"   Loaded {len(summaries_df)} summaries\")\n",
    "\n",
    "# Merge summaries into movies\n",
    "movies_df = movies_df.merge(summaries_df, on='movieId', how='left')\n",
    "\n",
    "print(\"\\n‚úÖ Summaries merged!\")\n",
    "print(f\"\\nüìù Sample summary for '{movies_df.iloc[0]['title']}':\")\n",
    "print(f\"   {movies_df.iloc[0]['summary'][:200]}...\" if pd.notna(movies_df.iloc[0]['summary']) else \"   (No summary available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Load and Merge Embeddings\n",
    "\n",
    "The embeddings are 1536-dimensional vectors generated by the Gemini embedding model. Each vector captures the semantic meaning of a movie's summary, enabling similarity search.\n",
    "\n",
    "**Why pre-computed embeddings?**\n",
    "- Generating embeddings for ~10,000 movies takes time\n",
    "- Pre-computing allows us to focus on AlloyDB features\n",
    "- In production, you'd typically compute embeddings when content is added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "print(\"üì• Loading embeddings.csv from GCS...\")\n",
    "embeddings_df = load_csv_from_gcs(DATA_BUCKET, \"embeddings.csv\")\n",
    "print(f\"   Loaded {len(embeddings_df)} embeddings\")\n",
    "\n",
    "# The embedding column contains JSON arrays as strings\n",
    "# We'll parse them when inserting into the database\n",
    "\n",
    "# Merge embeddings into movies\n",
    "movies_df = movies_df.merge(embeddings_df, on='movieId', how='left')\n",
    "\n",
    "print(\"\\n‚úÖ Embeddings merged!\")\n",
    "print(f\"\\nüî¢ Embedding dimensions: 1536\")\n",
    "print(f\"   First few values of first embedding: {movies_df.iloc[0]['embedding'][:50]}...\" if pd.notna(movies_df.iloc[0]['embedding']) else \"   (No embedding)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Insert Movies into AlloyDB\n",
    "\n",
    "Now we'll insert our prepared movie data into AlloyDB. Note how we handle the vector embeddings‚Äîthey're stored as JSON arrays and AlloyDB's vector extension handles the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare movies data for insertion\n",
    "conn = get_connection(DB_NAME)\n",
    "\n",
    "# Insert movies in batches for better performance\n",
    "insert_sql = \"\"\"\n",
    "    INSERT INTO movies (movie_id, title, year, summary, summary_embedding)\n",
    "    VALUES %s\n",
    "    ON CONFLICT (movie_id) DO UPDATE SET\n",
    "        title = EXCLUDED.title,\n",
    "        year = EXCLUDED.year,\n",
    "        summary = EXCLUDED.summary,\n",
    "        summary_embedding = EXCLUDED.summary_embedding\n",
    "\"\"\"\n",
    "\n",
    "# Prepare the data\n",
    "movies_data = []\n",
    "for _, row in movies_df.iterrows():\n",
    "    # Parse embedding from JSON string if present\n",
    "    embedding = None\n",
    "    if pd.notna(row.get('embedding')):\n",
    "        try:\n",
    "            embedding = row['embedding']  # Keep as string, PostgreSQL will parse it\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    movies_data.append((\n",
    "        int(row['movieId']),\n",
    "        row['title'],\n",
    "        int(row['year']) if pd.notna(row['year']) else None,\n",
    "        row.get('summary') if pd.notna(row.get('summary')) else None,\n",
    "        embedding\n",
    "    ))\n",
    "\n",
    "print(f\"üì§ Inserting {len(movies_data)} movies into AlloyDB...\")\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    execute_values(cur, insert_sql, movies_data, page_size=500)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"‚úÖ Movies inserted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Process and Load Genres\n",
    "\n",
    "MovieLens stores genres as pipe-delimited strings (e.g., \"Action|Comedy|Sci-Fi\"). We'll normalize this into a proper relational structure with a genres lookup table and a junction table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique genres\n",
    "all_genres = set()\n",
    "for genres_str in movies_with_genres['genres']:\n",
    "    if pd.notna(genres_str) and genres_str != '(no genres listed)':\n",
    "        all_genres.update(genres_str.split('|'))\n",
    "\n",
    "print(f\"üé¨ Found {len(all_genres)} unique genres:\")\n",
    "print(f\"   {', '.join(sorted(all_genres))}\")\n",
    "\n",
    "# Insert genres\n",
    "conn = get_connection(DB_NAME)\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    for genre in sorted(all_genres):\n",
    "        cur.execute(\n",
    "            \"INSERT INTO genres (genre_name) VALUES (%s) ON CONFLICT (genre_name) DO NOTHING\",\n",
    "            (genre,)\n",
    "        )\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "# Get genre IDs for the junction table\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"SELECT genre_id, genre_name FROM genres\")\n",
    "    genre_lookup = {name: id for id, name in cur.fetchall()}\n",
    "\n",
    "print(f\"\\n‚úÖ Genres inserted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create movie_genres junction records\n",
    "movie_genres_data = []\n",
    "for _, row in movies_with_genres.iterrows():\n",
    "    if pd.notna(row['genres']) and row['genres'] != '(no genres listed)':\n",
    "        movie_id = int(row['movieId'])\n",
    "        for genre in row['genres'].split('|'):\n",
    "            if genre in genre_lookup:\n",
    "                movie_genres_data.append((movie_id, genre_lookup[genre]))\n",
    "\n",
    "print(f\"üì§ Creating {len(movie_genres_data)} movie-genre associations...\")\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    execute_values(\n",
    "        cur,\n",
    "        \"INSERT INTO movie_genres (movie_id, genre_id) VALUES %s ON CONFLICT DO NOTHING\",\n",
    "        movie_genres_data,\n",
    "        page_size=1000\n",
    "    )\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"‚úÖ Movie-genre associations created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Load Ratings and Extract Users\n",
    "\n",
    "The ratings dataset contains over 100,000 ratings from 610 users. We'll first extract unique users, then load the ratings with their timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ratings\n",
    "print(\"üì• Loading ratings.csv from GCS...\")\n",
    "ratings_df = load_csv_from_gcs(DATA_BUCKET, \"ratings.csv\")\n",
    "print(f\"   Loaded {len(ratings_df)} ratings\")\n",
    "\n",
    "# Extract unique users\n",
    "unique_users = ratings_df['userId'].unique()\n",
    "print(f\"\\nüë• Found {len(unique_users)} unique users\")\n",
    "\n",
    "# Insert users\n",
    "conn = get_connection(DB_NAME)\n",
    "\n",
    "user_data = [(int(uid),) for uid in unique_users]\n",
    "with conn.cursor() as cur:\n",
    "    execute_values(\n",
    "        cur,\n",
    "        \"INSERT INTO users (user_id) VALUES %s ON CONFLICT DO NOTHING\",\n",
    "        user_data\n",
    "    )\n",
    "\n",
    "conn.commit()\n",
    "print(\"‚úÖ Users inserted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamps and prepare ratings data\n",
    "from datetime import datetime\n",
    "\n",
    "ratings_data = []\n",
    "for _, row in ratings_df.iterrows():\n",
    "    # Convert Unix timestamp to datetime\n",
    "    rated_at = datetime.fromtimestamp(row['timestamp'])\n",
    "    ratings_data.append((\n",
    "        int(row['userId']),\n",
    "        int(row['movieId']),\n",
    "        float(row['rating']),\n",
    "        rated_at\n",
    "    ))\n",
    "\n",
    "print(f\"üì§ Inserting {len(ratings_data)} ratings...\")\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    execute_values(\n",
    "        cur,\n",
    "        \"INSERT INTO ratings (user_id, movie_id, rating, rated_at) VALUES %s\",\n",
    "        ratings_data,\n",
    "        page_size=5000\n",
    "    )\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"‚úÖ Ratings inserted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7 Load Tags\n",
    "\n",
    "Tags are user-generated labels for movies. These are great for demonstrating AlloyDB's AI SQL functions later‚Äîwe can use AI to understand tag meanings and find semantically similar movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tags\n",
    "print(\"üì• Loading tags.csv from GCS...\")\n",
    "tags_df = load_csv_from_gcs(DATA_BUCKET, \"tags.csv\")\n",
    "print(f\"   Loaded {len(tags_df)} tags\")\n",
    "\n",
    "# Prepare tags data\n",
    "tags_data = []\n",
    "for _, row in tags_df.iterrows():\n",
    "    tagged_at = datetime.fromtimestamp(row['timestamp'])\n",
    "    tags_data.append((\n",
    "        int(row['userId']),\n",
    "        int(row['movieId']),\n",
    "        str(row['tag']),\n",
    "        tagged_at\n",
    "    ))\n",
    "\n",
    "print(f\"üì§ Inserting {len(tags_data)} tags...\")\n",
    "\n",
    "conn = get_connection(DB_NAME)\n",
    "with conn.cursor() as cur:\n",
    "    execute_values(\n",
    "        cur,\n",
    "        \"INSERT INTO tags (user_id, movie_id, tag_text, tagged_at) VALUES %s\",\n",
    "        tags_data,\n",
    "        page_size=1000\n",
    "    )\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"‚úÖ Tags inserted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Verify Your Data\n",
    "\n",
    "Let's make sure everything loaded correctly with some verification queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification queries\n",
    "conn = get_connection(DB_NAME)\n",
    "\n",
    "verification_queries = [\n",
    "    (\"movies\", \"SELECT COUNT(*) FROM movies\"),\n",
    "    (\"movies with summaries\", \"SELECT COUNT(*) FROM movies WHERE summary IS NOT NULL\"),\n",
    "    (\"movies with embeddings\", \"SELECT COUNT(*) FROM movies WHERE summary_embedding IS NOT NULL\"),\n",
    "    (\"genres\", \"SELECT COUNT(*) FROM genres\"),\n",
    "    (\"movie_genres\", \"SELECT COUNT(*) FROM movie_genres\"),\n",
    "    (\"users\", \"SELECT COUNT(*) FROM users\"),\n",
    "    (\"ratings\", \"SELECT COUNT(*) FROM ratings\"),\n",
    "    (\"tags\", \"SELECT COUNT(*) FROM tags\"),\n",
    "]\n",
    "\n",
    "print(\"üìä Data Verification Report\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    for name, query in verification_queries:\n",
    "        cur.execute(query)\n",
    "        count = cur.fetchone()[0]\n",
    "        print(f\"   {name}: {count:,}\")\n",
    "\n",
    "conn.close()\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample query: Top-rated movies with their genres\n",
    "sample_query = \"\"\"\n",
    "SELECT \n",
    "    m.title,\n",
    "    m.year,\n",
    "    ROUND(AVG(r.rating), 2) as avg_rating,\n",
    "    COUNT(r.rating_id) as num_ratings,\n",
    "    STRING_AGG(DISTINCT g.genre_name, ', ' ORDER BY g.genre_name) as genres\n",
    "FROM movies m\n",
    "JOIN ratings r ON m.movie_id = r.movie_id\n",
    "JOIN movie_genres mg ON m.movie_id = mg.movie_id\n",
    "JOIN genres g ON mg.genre_id = g.genre_id\n",
    "GROUP BY m.movie_id, m.title, m.year\n",
    "HAVING COUNT(r.rating_id) >= 50\n",
    "ORDER BY avg_rating DESC, num_ratings DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "conn = get_connection(DB_NAME)\n",
    "result_df = pd.read_sql(sample_query, conn)\n",
    "conn.close()\n",
    "\n",
    "print(\"\\nüèÜ Top 10 Highest-Rated Movies (minimum 50 ratings):\")\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéâ Congratulations!\n",
    "\n",
    "Your CymbalFlix database is ready! Here's what you've accomplished:\n",
    "\n",
    "‚úÖ Connected to AlloyDB using **IAM authentication** (no passwords!)  \n",
    "‚úÖ Created a dedicated database with AI extensions enabled  \n",
    "‚úÖ Built a normalized schema for movies, users, ratings, and tags  \n",
    "‚úÖ Loaded and transformed MovieLens data  \n",
    "‚úÖ Added AI-generated summaries and vector embeddings  \n",
    "\n",
    "### Security Note\n",
    "\n",
    "Notice how we never handled a database password? That's IAM authentication in action:\n",
    "- Your Google Cloud identity IS your database identity\n",
    "- The Auth Proxy automatically manages secure token exchange\n",
    "- No credentials to rotate, leak, or manage\n",
    "\n",
    "This is the **production-ready** way to handle database authentication in Google Cloud.\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In the upcoming lab modules, you'll:\n",
    "\n",
    "1. **Create a ScaNN Index** - Enable lightning-fast vector similarity search\n",
    "2. **Run Semantic Searches** - Find movies by meaning, not just keywords\n",
    "3. **Use AI SQL Functions** - Apply Gemini intelligence directly in your queries\n",
    "4. **Enable the Columnar Engine** - Accelerate analytical queries by up to 100x\n",
    "\n",
    "Your database is now ready to power an AI-driven movie discovery experience! üé¨ü§ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: Stop the Auth Proxy when you're done\n",
    "# Uncomment the line below if you want to stop the proxy\n",
    "# proxy_process.terminate()\n",
    "# print(\"Auth Proxy stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
