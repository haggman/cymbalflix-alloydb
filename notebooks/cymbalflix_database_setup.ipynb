{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé¨ CymbalFlix Discover - Database Setup\n",
    "\n",
    "Welcome to the data engineering portion of CymbalFlix Discover! In this notebook, you'll set up your AlloyDB database with everything needed for an AI-powered movie discovery application.\n",
    "\n",
    "## What We're Building\n",
    "\n",
    "By the end of this notebook, your database will contain:\n",
    "\n",
    "| Table | Records | Purpose |\n",
    "|-------|---------|--------|\n",
    "| `movies` | ~9,700 | Core catalog with AI-searchable summaries and vector embeddings |\n",
    "| `genres` | 20 | Genre lookup table |\n",
    "| `movie_genres` | ~21,000 | Many-to-many junction for movie genres |\n",
    "| `users` | 610 | User profiles extracted from ratings data |\n",
    "| `ratings` | 100,836 | Historical ratings for analytics |\n",
    "| `tags` | 3,683 | User-generated tags for semantic analysis |\n",
    "| `links` | ~9,700 | External IDs (IMDb, TMDb) for integration |\n",
    "| `watchlist` | 0 | Ready for user watchlist operations |\n",
    "\n",
    "## AlloyDB Extensions We'll Enable\n",
    "\n",
    "- **`vector`** - PostgreSQL vector data type for embeddings\n",
    "- **`alloydb_scann`** - Google's ScaNN index for lightning-fast vector search\n",
    "- **`google_ml_integration`** - Direct Vertex AI access from SQL\n",
    "\n",
    "## Security: IAM Authentication\n",
    "\n",
    "Notice something missing? **No database passwords!** We're using IAM authentication, which means:\n",
    "- Your Google Cloud identity is your database identity\n",
    "- No passwords to manage, rotate, or accidentally commit to Git\n",
    "- The AlloyDB Python Connector handles secure authentication automatically\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Configure Your Environment\n",
    "\n",
    "First, let's set up the configuration for your specific AlloyDB cluster. Fill in the form fields below with values from your lab instructions.\n",
    "\n",
    "**Tip:** The form fields appear when you click on this cell. Just fill them in and run the cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Configuration - Fill in your lab details { display-mode: \"form\" }\n",
    "# @markdown Enter your project and cluster information from the lab instructions:\n",
    "\n",
    "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "CLUSTER_ID = \"cymbalflix-cluster\"  # @param {type:\"string\"}\n",
    "INSTANCE_ID = \"cymbalflix-primary\"  # @param {type:\"string\"}\n",
    "\n",
    "# Database name we'll create\n",
    "DB_NAME = \"cymbalflix\"\n",
    "\n",
    "# GCS bucket with our MovieLens data\n",
    "DATA_BUCKET = \"gs://class-demo/ml-latest-small\"\n",
    "\n",
    "# Validate configuration\n",
    "if not PROJECT_ID or PROJECT_ID == \"\":\n",
    "    print(\"‚ùå Please enter your PROJECT_ID in the form field above!\")\n",
    "    print(\"   You can find it in the lab instructions or Cloud Console.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Configuration set!\")\n",
    "    print(f\"   Project:  {PROJECT_ID}\")\n",
    "    print(f\"   Region:   {REGION}\")\n",
    "    print(f\"   Cluster:  {CLUSTER_ID}\")\n",
    "    print(f\"   Instance: {INSTANCE_ID}\")\n",
    "    print(f\"\\nüîê Using IAM authentication (no password required!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Install Dependencies & Connect to AlloyDB\n",
    "\n",
    "We'll use the **AlloyDB Python Connector** to establish a secure connection. This connector:\n",
    "\n",
    "- Handles IAM authentication automatically\n",
    "- Creates encrypted connections without manual certificate management  \n",
    "- Works seamlessly in Colab, Cloud Shell, or any Python environment\n",
    "- Is the recommended approach for production applications\n",
    "\n",
    "**Why not Auth Proxy?** The Python Connector is more reliable in notebook environments and eliminates the need to manage a separate proxy process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q google-cloud-alloydb-connector[pg8000] \\\n",
    "    pandas google-cloud-storage sqlalchemy\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "from google.cloud.alloydb.connector import Connector\n",
    "import pg8000\n",
    "import sqlalchemy\n",
    "from sqlalchemy import text\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Build the instance URI for the connector\n",
    "INSTANCE_URI = f\"projects/{PROJECT_ID}/locations/{REGION}/clusters/{CLUSTER_ID}/instances/{INSTANCE_ID}\"\n",
    "\n",
    "# Initialize the AlloyDB connector\n",
    "connector = Connector()\n",
    "\n",
    "def get_connection(database=\"postgres\"):\n",
    "    \"\"\"\n",
    "    Create a connection to AlloyDB using the Python Connector.\n",
    "    \n",
    "    With enable_iam_auth=True, your Google Cloud identity is used\n",
    "    for authentication - no password needed!\n",
    "    \"\"\"\n",
    "    conn = connector.connect(\n",
    "        INSTANCE_URI,\n",
    "        \"pg8000\",\n",
    "        db=database,\n",
    "        enable_iam_auth=True,  # Use your Google Cloud identity!\n",
    "    )\n",
    "    return conn\n",
    "\n",
    "# Test the connection\n",
    "print(f\"üîó Connecting to: {INSTANCE_URI}\")\n",
    "print(\"‚è≥ Establishing secure connection...\")\n",
    "\n",
    "try:\n",
    "    conn = get_connection()\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT version();\")\n",
    "    version = cursor.fetchone()[0]\n",
    "    cursor.execute(\"SELECT current_user;\")\n",
    "    current_user = cursor.fetchone()[0]\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"\\n‚úÖ Successfully connected to AlloyDB!\")\n",
    "    print(f\"\\nüîê Authenticated as: {current_user}\")\n",
    "    print(f\"\\nüìä Database version:\")\n",
    "    print(f\"   {version[:60]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Connection failed: {e}\")\n",
    "    print(\"\\nüîç Troubleshooting tips:\")\n",
    "    print(\"   1. Verify your PROJECT_ID is correct (check the form above)\")\n",
    "    print(\"   2. Make sure your AlloyDB cluster shows 'Ready' in Cloud Console\")\n",
    "    print(\"   3. Confirm the cluster and instance names match your Terraform output\")\n",
    "    print(\"   4. Check that your user has the AlloyDB IAM Database User role\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Create the CymbalFlix Database\n",
    "\n",
    "We'll create a dedicated database for CymbalFlix rather than using the default `postgres` database. This is a best practice‚Äîit keeps your application data isolated and makes it easier to manage permissions, backups, and migrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the cymbalflix database\n",
    "# We need to use autocommit mode for CREATE DATABASE\n",
    "conn = get_connection(\"postgres\")\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Check if database exists\n",
    "cursor.execute(\"SELECT 1 FROM pg_database WHERE datname = %s\", (DB_NAME,))\n",
    "exists = cursor.fetchone()\n",
    "\n",
    "if not exists:\n",
    "    cursor.execute(f\"CREATE DATABASE {DB_NAME}\")\n",
    "    print(f\"‚úÖ Created database: {DB_NAME}\")\n",
    "else:\n",
    "    print(f\"‚ÑπÔ∏è  Database '{DB_NAME}' already exists - continuing...\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Enable Extensions & Register AI Models\n",
    "\n",
    "This is where AlloyDB becomes more than just PostgreSQL! We'll enable three powerful extensions:\n",
    "\n",
    "| Extension | What It Does |\n",
    "|-----------|-------------|\n",
    "| `vector` | Adds the VECTOR data type for storing embeddings |\n",
    "| `alloydb_scann` | Enables Google's ScaNN algorithm for fast similarity search |\n",
    "| `google_ml_integration` | Connects AlloyDB directly to Vertex AI |\n",
    "\n",
    "We'll also register two Vertex AI model endpoints that AlloyDB can call directly:\n",
    "- **gemini-pro** - For AI SQL functions like `ai.if()` and `ai.rank()`\n",
    "- **text-embedding** - For generating embeddings from text queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable AlloyDB extensions\n",
    "conn = get_connection(DB_NAME)\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "\n",
    "extensions = [\n",
    "    (\"vector\", \"Vector data type for embeddings\"),\n",
    "    (\"alloydb_scann\", \"ScaNN index for lightning-fast vector similarity search\"),\n",
    "    (\"google_ml_integration\", \"Direct Vertex AI integration for AI SQL functions\")\n",
    "]\n",
    "\n",
    "print(\"üîß Enabling AlloyDB extensions...\\n\")\n",
    "\n",
    "for ext_name, description in extensions:\n",
    "    try:\n",
    "        cursor.execute(f\"CREATE EXTENSION IF NOT EXISTS {ext_name}\")\n",
    "        print(f\"‚úÖ {ext_name}\")\n",
    "        print(f\"   ‚îî‚îÄ {description}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not enable {ext_name}: {e}\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"\\nüéâ Extensions enabled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Vertex AI model endpoints\n",
    "conn = get_connection(DB_NAME)\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(\"ü§ñ Registering Vertex AI model endpoints...\\n\")\n",
    "\n",
    "# Register Gemini Pro for text generation (AI SQL functions)\n",
    "try:\n",
    "    cursor.execute(f\"\"\"\n",
    "        CALL google_ml.create_model(\n",
    "            model_id => 'gemini-pro',\n",
    "            model_request_url => 'https://{REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/publishers/google/models/gemini-2.0-flash:generateContent',\n",
    "            model_provider => 'google',\n",
    "            model_type => 'text_generation'\n",
    "        );\n",
    "    \"\"\")\n",
    "    print(\"‚úÖ gemini-pro (text generation)\")\n",
    "    print(\"   ‚îî‚îÄ Powers ai.if(), ai.rank(), and ai.generate() functions\")\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        print(\"‚ÑπÔ∏è  gemini-pro already registered\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Could not register gemini-pro: {e}\")\n",
    "\n",
    "# Register text-embedding model for generating embeddings\n",
    "try:\n",
    "    cursor.execute(f\"\"\"\n",
    "        CALL google_ml.create_model(\n",
    "            model_id => 'text-embedding',\n",
    "            model_request_url => 'https://{REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/publishers/google/models/text-embedding-005:predict',\n",
    "            model_provider => 'google',\n",
    "            model_type => 'text_embedding'\n",
    "        );\n",
    "    \"\"\")\n",
    "    print(\"\\n‚úÖ text-embedding (embedding generation)\")\n",
    "    print(\"   ‚îî‚îÄ Generates vectors for semantic search queries\")\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        print(\"‚ÑπÔ∏è  text-embedding already registered\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Could not register text-embedding: {e}\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"\\nüéâ AlloyDB is ready for AI-powered operations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Create the Database Schema\n",
    "\n",
    "Our schema is designed for both transactional operations (watchlists, ratings) and analytical queries (trending movies, genre analysis). \n",
    "\n",
    "**Key design decisions:**\n",
    "\n",
    "- **Normalized genres** - Instead of storing \"Action|Comedy|Sci-Fi\" as text, we use a proper junction table\n",
    "- **Vector column** - The `movies.summary_embedding` stores 1536-dimensional vectors for semantic search\n",
    "- **Foreign keys** - Enforce data integrity across related tables\n",
    "- **Timestamps** - Enable temporal analysis and audit trails\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   movies    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ movie_genres ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ   genres    ‚îÇ\n",
    "‚îÇ (+ vector)  ‚îÇ       ‚îÇ  (junction)  ‚îÇ       ‚îÇ  (lookup)   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "       ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "       ‚îÇ                    ‚îÇ                     ‚îÇ\n",
    "       ‚ñº                    ‚ñº                     ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   ratings   ‚îÇ       ‚îÇ    tags     ‚îÇ       ‚îÇ   links     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ                    ‚îÇ\n",
    "       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                ‚ñº\n",
    "          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "          ‚îÇ    users    ‚îÇ\n",
    "          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                ‚îÇ\n",
    "                ‚ñº\n",
    "          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "          ‚îÇ  watchlist  ‚îÇ\n",
    "          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our database schema\n",
    "schema_sql = \"\"\"\n",
    "-- Core movie catalog with vector embeddings for semantic search\n",
    "CREATE TABLE IF NOT EXISTS movies (\n",
    "    movie_id INTEGER PRIMARY KEY,\n",
    "    title VARCHAR(255) NOT NULL,\n",
    "    year INTEGER,\n",
    "    summary TEXT,\n",
    "    summary_embedding VECTOR(1536)\n",
    ");\n",
    "\n",
    "-- Genre lookup table\n",
    "CREATE TABLE IF NOT EXISTS genres (\n",
    "    genre_id SERIAL PRIMARY KEY,\n",
    "    genre_name VARCHAR(50) UNIQUE NOT NULL\n",
    ");\n",
    "\n",
    "-- Many-to-many junction table for movie genres\n",
    "CREATE TABLE IF NOT EXISTS movie_genres (\n",
    "    movie_id INTEGER REFERENCES movies(movie_id) ON DELETE CASCADE,\n",
    "    genre_id INTEGER REFERENCES genres(genre_id) ON DELETE CASCADE,\n",
    "    PRIMARY KEY (movie_id, genre_id)\n",
    ");\n",
    "\n",
    "-- User profiles (extracted from ratings data)\n",
    "CREATE TABLE IF NOT EXISTS users (\n",
    "    user_id INTEGER PRIMARY KEY,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\n",
    "-- Historical ratings for analytics\n",
    "CREATE TABLE IF NOT EXISTS ratings (\n",
    "    rating_id SERIAL PRIMARY KEY,\n",
    "    user_id INTEGER REFERENCES users(user_id) ON DELETE CASCADE,\n",
    "    movie_id INTEGER REFERENCES movies(movie_id) ON DELETE CASCADE,\n",
    "    rating NUMERIC(2,1) NOT NULL CHECK (rating >= 0.5 AND rating <= 5.0),\n",
    "    rated_at TIMESTAMP\n",
    ");\n",
    "\n",
    "-- User-generated tags for semantic analysis\n",
    "CREATE TABLE IF NOT EXISTS tags (\n",
    "    tag_id SERIAL PRIMARY KEY,\n",
    "    user_id INTEGER REFERENCES users(user_id) ON DELETE CASCADE,\n",
    "    movie_id INTEGER REFERENCES movies(movie_id) ON DELETE CASCADE,\n",
    "    tag_text VARCHAR(255) NOT NULL,\n",
    "    tagged_at TIMESTAMP\n",
    ");\n",
    "\n",
    "-- User watchlists (for transactional operations)\n",
    "CREATE TABLE IF NOT EXISTS watchlist (\n",
    "    user_id INTEGER REFERENCES users(user_id) ON DELETE CASCADE,\n",
    "    movie_id INTEGER REFERENCES movies(movie_id) ON DELETE CASCADE,\n",
    "    added_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    PRIMARY KEY (user_id, movie_id)\n",
    ");\n",
    "\n",
    "-- External database links (IMDb, TMDb)\n",
    "CREATE TABLE IF NOT EXISTS links (\n",
    "    movie_id INTEGER PRIMARY KEY REFERENCES movies(movie_id) ON DELETE CASCADE,\n",
    "    imdb_id VARCHAR(20),\n",
    "    tmdb_id INTEGER\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Execute the schema\n",
    "conn = get_connection(DB_NAME)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(schema_sql)\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"‚úÖ Database schema created!\")\n",
    "print(\"\\nüìã Tables created:\")\n",
    "print(\"   ‚Ä¢ movies (with VECTOR(1536) for embeddings)\")\n",
    "print(\"   ‚Ä¢ genres\")\n",
    "print(\"   ‚Ä¢ movie_genres (junction table)\")\n",
    "print(\"   ‚Ä¢ users\")\n",
    "print(\"   ‚Ä¢ ratings\")\n",
    "print(\"   ‚Ä¢ tags\")\n",
    "print(\"   ‚Ä¢ watchlist\")\n",
    "print(\"   ‚Ä¢ links (IMDb/TMDb IDs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Load Data from Google Cloud Storage\n",
    "\n",
    "Now comes the fun part‚Äîloading our MovieLens data! We'll load data directly from GCS and transform it as we go:\n",
    "\n",
    "1. **Movies** - Extract year from title, e.g., \"Toy Story (1995)\" ‚Üí title=\"Toy Story\", year=1995\n",
    "2. **Summaries** - AI-generated movie descriptions (merge into movies)\n",
    "3. **Embeddings** - Pre-computed 1536-dimensional vectors from Gemini\n",
    "4. **Genres** - Parse pipe-delimited genres into a normalized structure\n",
    "5. **Users** - Extract unique user IDs from ratings\n",
    "6. **Ratings & Tags** - Load with timestamp conversion\n",
    "7. **Links** - External database identifiers\n",
    "\n",
    "Let's start with a helper function to load CSV files from GCS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_from_gcs(bucket_path, filename):\n",
    "    \"\"\"Load a CSV file from GCS into a pandas DataFrame.\"\"\"\n",
    "    # Parse the bucket path (handle gs:// prefix and nested paths)\n",
    "    path = bucket_path\n",
    "    if path.startswith(\"gs://\"):\n",
    "        path = path[5:]\n",
    "    \n",
    "    if \"/\" in path:\n",
    "        parts = path.split(\"/\", 1)\n",
    "        bucket_name = parts[0]\n",
    "        blob_path = f\"{parts[1]}/{filename}\"\n",
    "    else:\n",
    "        bucket_name = path\n",
    "        blob_path = filename\n",
    "    \n",
    "    client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_path)\n",
    "    \n",
    "    content = blob.download_as_text()\n",
    "    return pd.read_csv(io.StringIO(content))\n",
    "\n",
    "print(\"‚úÖ GCS loader ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Load and Transform Movies\n",
    "\n",
    "The MovieLens dataset stores the year in the title (e.g., \"Jumanji (1995)\"). We'll extract it into a separate column for better querying and analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load movies from GCS\n",
    "print(\"üì• Loading movies.csv from GCS...\")\n",
    "movies_df = load_csv_from_gcs(DATA_BUCKET, \"movies.csv\")\n",
    "print(f\"   Loaded {len(movies_df):,} movies\")\n",
    "\n",
    "# Extract year from title using regex\n",
    "# Pattern matches \" (YYYY)\" at the end of the title\n",
    "def extract_year_and_clean_title(title):\n",
    "    match = re.search(r'\\s*\\((\\d{4})\\)\\s*$', str(title))\n",
    "    if match:\n",
    "        year = int(match.group(1))\n",
    "        clean_title = re.sub(r'\\s*\\(\\d{4}\\)\\s*$', '', title).strip()\n",
    "        return clean_title, year\n",
    "    return title, None\n",
    "\n",
    "# Apply the transformation\n",
    "movies_df[['clean_title', 'year']] = movies_df['title'].apply(\n",
    "    lambda x: pd.Series(extract_year_and_clean_title(x))\n",
    ")\n",
    "movies_df['title'] = movies_df['clean_title']\n",
    "movies_df = movies_df.drop(columns=['clean_title'])\n",
    "\n",
    "# Store genres for later processing\n",
    "movies_with_genres = movies_df[['movieId', 'genres']].copy()\n",
    "\n",
    "print(\"\\n‚úÖ Movies processed!\")\n",
    "print(f\"\\nüìä Sample data:\")\n",
    "display(movies_df[['movieId', 'title', 'year']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Load and Merge Summaries\n",
    "\n",
    "The summaries were generated using Gemini to provide rich, searchable descriptions of each movie. These enable semantic search‚Äîfinding movies based on meaning, not just keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load summaries\n",
    "print(\"üì• Loading summaries.csv from GCS...\")\n",
    "summaries_df = load_csv_from_gcs(DATA_BUCKET, \"summaries.csv\")\n",
    "print(f\"   Loaded {len(summaries_df):,} summaries\")\n",
    "\n",
    "# Merge summaries into movies\n",
    "movies_df = movies_df.merge(summaries_df, on='movieId', how='left')\n",
    "\n",
    "print(\"\\n‚úÖ Summaries merged!\")\n",
    "\n",
    "# Show a sample summary\n",
    "sample_movie = movies_df.iloc[0]\n",
    "if pd.notna(sample_movie.get('summary')):\n",
    "    print(f\"\\nüìù Sample summary for '{sample_movie['title']}':\")\n",
    "    print(f\"   {sample_movie['summary'][:250]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Load and Merge Embeddings\n",
    "\n",
    "The embeddings are 1536-dimensional vectors generated by Gemini's embedding model. Each vector captures the semantic meaning of a movie's summary, enabling similarity search.\n",
    "\n",
    "**Why 1536 dimensions?** That's what Gemini's `text-embedding-005` model produces. More dimensions can capture more nuance, but also require more storage and computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "print(\"üì• Loading embeddings.csv from GCS...\")\n",
    "embeddings_df = load_csv_from_gcs(DATA_BUCKET, \"embeddings.csv\")\n",
    "print(f\"   Loaded {len(embeddings_df):,} embeddings\")\n",
    "\n",
    "# Merge embeddings into movies\n",
    "movies_df = movies_df.merge(embeddings_df, on='movieId', how='left')\n",
    "\n",
    "# Verify embedding format\n",
    "sample_embedding = movies_df.iloc[0].get('embedding')\n",
    "if pd.notna(sample_embedding):\n",
    "    # Parse the JSON array to check dimensions\n",
    "    try:\n",
    "        embedding_values = json.loads(sample_embedding)\n",
    "        print(f\"\\n‚úÖ Embeddings merged!\")\n",
    "        print(f\"\\nüî¢ Embedding details:\")\n",
    "        print(f\"   Dimensions: {len(embedding_values)}\")\n",
    "        print(f\"   Sample values: [{embedding_values[0]:.6f}, {embedding_values[1]:.6f}, ...]\")\n",
    "    except:\n",
    "        print(\"\\n‚úÖ Embeddings merged (format will be parsed during insert)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Insert Movies into AlloyDB\n",
    "\n",
    "Now we'll insert our prepared movie data into AlloyDB. The vector embeddings are stored as JSON arrays‚ÄîAlloyDB's vector extension handles the conversion automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare movies for insertion\n",
    "conn = get_connection(DB_NAME)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(f\"üì§ Inserting {len(movies_df):,} movies into AlloyDB...\")\n",
    "\n",
    "# Insert movies with upsert logic\n",
    "insert_count = 0\n",
    "for _, row in movies_df.iterrows():\n",
    "    try:\n",
    "        # Handle embedding - it's stored as a JSON string\n",
    "        embedding = None\n",
    "        if pd.notna(row.get('embedding')):\n",
    "            embedding = row['embedding']  # Keep as string for PostgreSQL\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO movies (movie_id, title, year, summary, summary_embedding)\n",
    "            VALUES (%s, %s, %s, %s, %s)\n",
    "            ON CONFLICT (movie_id) DO UPDATE SET\n",
    "                title = EXCLUDED.title,\n",
    "                year = EXCLUDED.year,\n",
    "                summary = EXCLUDED.summary,\n",
    "                summary_embedding = EXCLUDED.summary_embedding\n",
    "        \"\"\", (\n",
    "            int(row['movieId']),\n",
    "            row['title'],\n",
    "            int(row['year']) if pd.notna(row['year']) else None,\n",
    "            row.get('summary') if pd.notna(row.get('summary')) else None,\n",
    "            embedding\n",
    "        ))\n",
    "        insert_count += 1\n",
    "        \n",
    "        # Progress indicator\n",
    "        if insert_count % 2000 == 0:\n",
    "            print(f\"   Processed {insert_count:,} movies...\")\n",
    "            conn.commit()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Error inserting movie {row['movieId']}: {e}\")\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(f\"\\n‚úÖ Inserted {insert_count:,} movies successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Process and Load Genres\n",
    "\n",
    "MovieLens stores genres as pipe-delimited strings (e.g., \"Action|Comedy|Sci-Fi\"). We'll normalize this into a proper relational structure with:\n",
    "- A `genres` lookup table with unique genre names\n",
    "- A `movie_genres` junction table linking movies to their genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique genres\n",
    "all_genres = set()\n",
    "for genres_str in movies_with_genres['genres']:\n",
    "    if pd.notna(genres_str) and genres_str != '(no genres listed)':\n",
    "        all_genres.update(genres_str.split('|'))\n",
    "\n",
    "print(f\"üé¨ Found {len(all_genres)} unique genres:\")\n",
    "print(f\"   {', '.join(sorted(all_genres))}\")\n",
    "\n",
    "# Insert genres into lookup table\n",
    "conn = get_connection(DB_NAME)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "for genre in sorted(all_genres):\n",
    "    cursor.execute(\n",
    "        \"INSERT INTO genres (genre_name) VALUES (%s) ON CONFLICT (genre_name) DO NOTHING\",\n",
    "        (genre,)\n",
    "    )\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "# Get genre IDs for the junction table\n",
    "cursor.execute(\"SELECT genre_id, genre_name FROM genres\")\n",
    "genre_lookup = {name: gid for gid, name in cursor.fetchall()}\n",
    "\n",
    "print(f\"\\n‚úÖ Genres inserted into lookup table!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create movie_genres junction records\n",
    "print(\"üì§ Creating movie-genre associations...\")\n",
    "\n",
    "junction_count = 0\n",
    "for _, row in movies_with_genres.iterrows():\n",
    "    if pd.notna(row['genres']) and row['genres'] != '(no genres listed)':\n",
    "        movie_id = int(row['movieId'])\n",
    "        for genre in row['genres'].split('|'):\n",
    "            if genre in genre_lookup:\n",
    "                try:\n",
    "                    cursor.execute(\n",
    "                        \"INSERT INTO movie_genres (movie_id, genre_id) VALUES (%s, %s) ON CONFLICT DO NOTHING\",\n",
    "                        (movie_id, genre_lookup[genre])\n",
    "                    )\n",
    "                    junction_count += 1\n",
    "                except Exception as e:\n",
    "                    pass  # Skip if movie doesn't exist\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(f\"\\n‚úÖ Created {junction_count:,} movie-genre associations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Load Users and Ratings\n",
    "\n",
    "The ratings dataset contains over 100,000 ratings from 610 users. We'll:\n",
    "1. Extract unique user IDs and create user records\n",
    "2. Load ratings with converted timestamps (Unix epoch ‚Üí PostgreSQL timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ratings\n",
    "print(\"üì• Loading ratings.csv from GCS...\")\n",
    "ratings_df = load_csv_from_gcs(DATA_BUCKET, \"ratings.csv\")\n",
    "print(f\"   Loaded {len(ratings_df):,} ratings\")\n",
    "\n",
    "# Extract and insert unique users\n",
    "unique_users = ratings_df['userId'].unique()\n",
    "print(f\"\\nüë• Found {len(unique_users):,} unique users\")\n",
    "\n",
    "conn = get_connection(DB_NAME)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "for uid in unique_users:\n",
    "    cursor.execute(\n",
    "        \"INSERT INTO users (user_id) VALUES (%s) ON CONFLICT DO NOTHING\",\n",
    "        (int(uid),)\n",
    "    )\n",
    "\n",
    "conn.commit()\n",
    "print(\"‚úÖ Users inserted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert ratings with timestamp conversion\n",
    "print(f\"üì§ Inserting {len(ratings_df):,} ratings...\")\n",
    "\n",
    "rating_count = 0\n",
    "for _, row in ratings_df.iterrows():\n",
    "    try:\n",
    "        rated_at = datetime.fromtimestamp(row['timestamp'])\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO ratings (user_id, movie_id, rating, rated_at)\n",
    "            VALUES (%s, %s, %s, %s)\n",
    "        \"\"\", (\n",
    "            int(row['userId']),\n",
    "            int(row['movieId']),\n",
    "            float(row['rating']),\n",
    "            rated_at\n",
    "        ))\n",
    "        rating_count += 1\n",
    "        \n",
    "        if rating_count % 20000 == 0:\n",
    "            print(f\"   Processed {rating_count:,} ratings...\")\n",
    "            conn.commit()\n",
    "            \n",
    "    except Exception as e:\n",
    "        pass  # Skip ratings for movies that don't exist\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(f\"\\n‚úÖ Inserted {rating_count:,} ratings!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7 Load Tags\n",
    "\n",
    "Tags are user-generated labels for movies‚Äîthings like \"twist ending\", \"based on a book\", or \"visually stunning\". These are great for demonstrating AlloyDB's AI SQL functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tags\n",
    "print(\"üì• Loading tags.csv from GCS...\")\n",
    "tags_df = load_csv_from_gcs(DATA_BUCKET, \"tags.csv\")\n",
    "print(f\"   Loaded {len(tags_df):,} tags\")\n",
    "\n",
    "conn = get_connection(DB_NAME)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(f\"üì§ Inserting tags...\")\n",
    "\n",
    "tag_count = 0\n",
    "for _, row in tags_df.iterrows():\n",
    "    try:\n",
    "        tagged_at = datetime.fromtimestamp(row['timestamp'])\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO tags (user_id, movie_id, tag_text, tagged_at)\n",
    "            VALUES (%s, %s, %s, %s)\n",
    "        \"\"\", (\n",
    "            int(row['userId']),\n",
    "            int(row['movieId']),\n",
    "            str(row['tag']),\n",
    "            tagged_at\n",
    "        ))\n",
    "        tag_count += 1\n",
    "    except Exception as e:\n",
    "        pass  # Skip tags for movies that don't exist\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(f\"\\n‚úÖ Inserted {tag_count:,} tags!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.8 Load Links\n",
    "\n",
    "The links file contains external database identifiers for each movie:\n",
    "- **IMDb ID** - Used for linking to IMDb pages (format: tt0000000)\n",
    "- **TMDb ID** - The Movie Database ID for accessing additional metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load links\n",
    "print(\"üì• Loading links.csv from GCS...\")\n",
    "links_df = load_csv_from_gcs(DATA_BUCKET, \"links.csv\")\n",
    "print(f\"   Loaded {len(links_df):,} links\")\n",
    "\n",
    "conn = get_connection(DB_NAME)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(f\"üì§ Inserting external links...\")\n",
    "\n",
    "link_count = 0\n",
    "for _, row in links_df.iterrows():\n",
    "    try:\n",
    "        # Format IMDb ID with leading zeros (tt0000000 format)\n",
    "        imdb_id = None\n",
    "        if pd.notna(row.get('imdbId')):\n",
    "            imdb_id = f\"tt{int(row['imdbId']):07d}\"\n",
    "        \n",
    "        tmdb_id = None\n",
    "        if pd.notna(row.get('tmdbId')):\n",
    "            tmdb_id = int(row['tmdbId'])\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO links (movie_id, imdb_id, tmdb_id)\n",
    "            VALUES (%s, %s, %s)\n",
    "            ON CONFLICT (movie_id) DO UPDATE SET\n",
    "                imdb_id = EXCLUDED.imdb_id,\n",
    "                tmdb_id = EXCLUDED.tmdb_id\n",
    "        \"\"\", (\n",
    "            int(row['movieId']),\n",
    "            imdb_id,\n",
    "            tmdb_id\n",
    "        ))\n",
    "        link_count += 1\n",
    "    except Exception as e:\n",
    "        pass  # Skip links for movies that don't exist\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(f\"\\n‚úÖ Inserted {link_count:,} external links!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Verify Your Data\n",
    "\n",
    "Let's make sure everything loaded correctly with some verification queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification queries\n",
    "conn = get_connection(DB_NAME)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "verification_queries = [\n",
    "    (\"movies\", \"SELECT COUNT(*) FROM movies\"),\n",
    "    (\"  ‚îî‚îÄ with summaries\", \"SELECT COUNT(*) FROM movies WHERE summary IS NOT NULL\"),\n",
    "    (\"  ‚îî‚îÄ with embeddings\", \"SELECT COUNT(*) FROM movies WHERE summary_embedding IS NOT NULL\"),\n",
    "    (\"genres\", \"SELECT COUNT(*) FROM genres\"),\n",
    "    (\"movie_genres\", \"SELECT COUNT(*) FROM movie_genres\"),\n",
    "    (\"users\", \"SELECT COUNT(*) FROM users\"),\n",
    "    (\"ratings\", \"SELECT COUNT(*) FROM ratings\"),\n",
    "    (\"tags\", \"SELECT COUNT(*) FROM tags\"),\n",
    "    (\"links\", \"SELECT COUNT(*) FROM links\"),\n",
    "]\n",
    "\n",
    "print(\"üìä Data Verification Report\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "for name, query in verification_queries:\n",
    "    cursor.execute(query)\n",
    "    count = cursor.fetchone()[0]\n",
    "    print(f\"   {name}: {count:,}\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"=\" * 45)\n",
    "print(\"\\n‚úÖ All data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample query: Top-rated movies with their genres\n",
    "sample_query = \"\"\"\n",
    "SELECT \n",
    "    m.title,\n",
    "    m.year,\n",
    "    ROUND(AVG(r.rating)::numeric, 2) as avg_rating,\n",
    "    COUNT(r.rating_id) as num_ratings,\n",
    "    STRING_AGG(DISTINCT g.genre_name, ', ' ORDER BY g.genre_name) as genres\n",
    "FROM movies m\n",
    "JOIN ratings r ON m.movie_id = r.movie_id\n",
    "JOIN movie_genres mg ON m.movie_id = mg.movie_id\n",
    "JOIN genres g ON mg.genre_id = g.genre_id\n",
    "GROUP BY m.movie_id, m.title, m.year\n",
    "HAVING COUNT(r.rating_id) >= 50\n",
    "ORDER BY avg_rating DESC, num_ratings DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "conn = get_connection(DB_NAME)\n",
    "result_df = pd.read_sql(sample_query, conn)\n",
    "conn.close()\n",
    "\n",
    "print(\"üèÜ Top 10 Highest-Rated Movies (minimum 50 ratings):\")\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Create the ScaNN Index\n",
    "\n",
    "Now for the feature that makes AlloyDB special for AI workloads‚Äîthe **ScaNN index**.\n",
    "\n",
    "**What is ScaNN?** Scalable Nearest Neighbors is Google's algorithm for fast vector similarity search. It's the same technology that powers Google Search's ability to find similar content across billions of documents.\n",
    "\n",
    "**Why do we need it?** Without an index, finding similar movies requires comparing your query vector against every single movie‚Äîthat's 9,700 comparisons. With ScaNN, the search narrows to a small subset almost instantly.\n",
    "\n",
    "| Without ScaNN | With ScaNN |\n",
    "|--------------|------------|\n",
    "| Compare against all 9,700 movies | Compare against ~50 candidates |\n",
    "| Linear time O(n) | Logarithmic time O(log n) |\n",
    "| ~100ms per query | ~5ms per query |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ScaNN index\n",
    "conn = get_connection(DB_NAME)\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(\"üîß Creating ScaNN index on movie embeddings...\")\n",
    "print(\"   This may take a moment...\\n\")\n",
    "\n",
    "try:\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE INDEX IF NOT EXISTS movies_embedding_scann_idx \n",
    "        ON movies USING scann (summary_embedding cosine)\n",
    "        WITH (num_leaves = 50, quantizer = 'sq8');\n",
    "    \"\"\")\n",
    "    print(\"‚úÖ ScaNN index created!\")\n",
    "    print(\"\\nüìä Index configuration:\")\n",
    "    print(\"   ‚Ä¢ Distance metric: cosine (measures angle between vectors)\")\n",
    "    print(\"   ‚Ä¢ num_leaves: 50 (partitions for efficient search)\")\n",
    "    print(\"   ‚Ä¢ quantizer: sq8 (8-bit scalar quantization for speed)\")\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        print(\"‚ÑπÔ∏è  ScaNN index already exists\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Could not create index: {e}\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Semantic Search Demo üéØ\n",
    "\n",
    "This is the payoff! Let's see semantic search in action.\n",
    "\n",
    "**How it works:**\n",
    "1. Your search query gets converted to a 1536-dimensional vector using Gemini's embedding model\n",
    "2. AlloyDB uses the ScaNN index to find movies with similar vectors\n",
    "3. Results are ranked by cosine similarity (1.0 = identical, 0.0 = completely different)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, limit=5):\n",
    "    \"\"\"\n",
    "    Search for movies using semantic similarity.\n",
    "    \n",
    "    This converts your natural language query into a vector,\n",
    "    then finds movies with similar vectors.\n",
    "    \"\"\"\n",
    "    conn = get_connection(DB_NAME)\n",
    "    \n",
    "    search_sql = \"\"\"\n",
    "    WITH query_embedding AS (\n",
    "        SELECT google_ml.embedding(\n",
    "            model_id => 'text-embedding',\n",
    "            content => %s\n",
    "        )::vector AS embedding\n",
    "    )\n",
    "    SELECT \n",
    "        m.title,\n",
    "        m.year,\n",
    "        ROUND((1 - (m.summary_embedding <=> q.embedding))::numeric, 3) as similarity,\n",
    "        LEFT(m.summary, 150) || '...' as summary_preview\n",
    "    FROM movies m\n",
    "    CROSS JOIN query_embedding q\n",
    "    WHERE m.summary_embedding IS NOT NULL\n",
    "    ORDER BY m.summary_embedding <=> q.embedding\n",
    "    LIMIT %s;\n",
    "    \"\"\"\n",
    "    \n",
    "    result = pd.read_sql(search_sql, conn, params=(query, limit))\n",
    "    conn.close()\n",
    "    return result\n",
    "\n",
    "print(\"‚úÖ Semantic search function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 1: Conceptual search\n",
    "print(\"üîç Search: 'A movie about artificial intelligence becoming self-aware'\")\n",
    "print(\"=\" * 70)\n",
    "results = semantic_search(\"A movie about artificial intelligence becoming self-aware\")\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 2: Emotional/thematic search\n",
    "print(\"üîç Search: 'Heartwarming story about unlikely friendship'\")\n",
    "print(\"=\" * 70)\n",
    "results = semantic_search(\"Heartwarming story about unlikely friendship\")\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 3: Compare semantic vs. what keyword search would find\n",
    "print(\"üîç Search: 'space adventure'\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüìä Semantic Search Results (finds movies by MEANING):\")\n",
    "results = semantic_search(\"space adventure\")\n",
    "display(results)\n",
    "\n",
    "# Now show what a simple keyword search would find\n",
    "print(\"\\nüìä Traditional Keyword Search (finds movies by EXACT WORDS):\")\n",
    "conn = get_connection(DB_NAME)\n",
    "keyword_results = pd.read_sql(\"\"\"\n",
    "    SELECT title, year, LEFT(summary, 100) || '...' as summary_preview\n",
    "    FROM movies \n",
    "    WHERE LOWER(title) LIKE '%space%' \n",
    "       OR LOWER(summary) LIKE '%space adventure%'\n",
    "    LIMIT 5;\n",
    "\"\"\", conn)\n",
    "conn.close()\n",
    "display(keyword_results)\n",
    "\n",
    "print(\"\\nüí° Notice how semantic search finds thematically similar movies\")\n",
    "print(\"   even if 'space adventure' doesn't appear in the text!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10: AI SQL Functions Demo (Optional)\n",
    "\n",
    "AlloyDB includes experimental AI SQL functions that bring Gemini's intelligence directly into your queries. These functions let you filter and rank data using natural language criteria.\n",
    "\n",
    "‚ö†Ô∏è **Note:** These functions are newer features and may not be available in all environments. If they produce errors, don't worry‚Äîthe semantic search you just tested is the primary feature for CymbalFlix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try ai.if() - Natural language filtering\n",
    "print(\"ü§ñ Testing AI SQL Functions...\\n\")\n",
    "\n",
    "conn = get_connection(DB_NAME)\n",
    "\n",
    "try:\n",
    "    print(\"üìä ai.if() Demo: Finding movies with happy endings\")\n",
    "    print(\"   Query: WHERE ai.if('this movie has a happy ending', summary)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    result = pd.read_sql(\"\"\"\n",
    "        SELECT title, year, LEFT(summary, 100) || '...' as summary_preview\n",
    "        FROM movies \n",
    "        WHERE summary IS NOT NULL\n",
    "          AND ai.if('this movie has a happy ending', summary)\n",
    "        LIMIT 5;\n",
    "    \"\"\", conn)\n",
    "    display(result)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  ai.if() not available in this environment: {e}\")\n",
    "    print(\"   This is okay! Semantic search still works perfectly.\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try ai.rank() - Relevance scoring\n",
    "conn = get_connection(DB_NAME)\n",
    "\n",
    "try:\n",
    "    print(\"üìä ai.rank() Demo: Scoring movies by family-friendliness\")\n",
    "    print(\"   Query: SELECT ai.rank('family-friendly', summary) as score\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    result = pd.read_sql(\"\"\"\n",
    "        SELECT \n",
    "            title, \n",
    "            year,\n",
    "            ai.rank('family-friendly and appropriate for children', summary) as family_score\n",
    "        FROM movies \n",
    "        WHERE summary IS NOT NULL\n",
    "        ORDER BY family_score DESC\n",
    "        LIMIT 10;\n",
    "    \"\"\", conn)\n",
    "    display(result)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  ai.rank() not available in this environment: {e}\")\n",
    "    print(\"   This is okay! Semantic search still works perfectly.\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 11: Verify Columnar Engine\n",
    "\n",
    "AlloyDB's columnar engine accelerates analytical queries by up to 100x. It works automatically‚ÄîAlloyDB identifies analytical query patterns and creates optimized columnar representations.\n",
    "\n",
    "Let's verify it's enabled on your instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check columnar engine settings\n",
    "conn = get_connection(DB_NAME)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(\"üîß Columnar Engine Configuration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT name, setting, short_desc \n",
    "    FROM pg_settings \n",
    "    WHERE name LIKE '%columnar%' OR name LIKE '%google_columnar%'\n",
    "    ORDER BY name;\n",
    "\"\"\")\n",
    "\n",
    "results = cursor.fetchall()\n",
    "if results:\n",
    "    for name, setting, desc in results:\n",
    "        print(f\"   {name}: {setting}\")\n",
    "    print(\"\\n‚úÖ Columnar engine is configured!\")\n",
    "    print(\"   Analytical queries will be automatically accelerated.\")\n",
    "else:\n",
    "    print(\"   No columnar settings found (may be auto-configured)\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéâ Congratulations!\n",
    "\n",
    "Your CymbalFlix database is fully operational! Here's what you've accomplished:\n",
    "\n",
    "### Database Setup\n",
    "- ‚úÖ Connected to AlloyDB using **IAM authentication** (no passwords!)\n",
    "- ‚úÖ Created a dedicated `cymbalflix` database\n",
    "- ‚úÖ Enabled vector, ScaNN, and ML integration extensions\n",
    "- ‚úÖ Registered Vertex AI model endpoints\n",
    "\n",
    "### Data Loading\n",
    "- ‚úÖ Loaded ~9,700 movies with AI-generated summaries\n",
    "- ‚úÖ Added 1536-dimensional vector embeddings for semantic search\n",
    "- ‚úÖ Normalized genres into a proper relational structure\n",
    "- ‚úÖ Loaded 100,000+ ratings and 3,600+ tags\n",
    "- ‚úÖ Added external links (IMDb, TMDb)\n",
    "\n",
    "### AI Features\n",
    "- ‚úÖ Created a ScaNN index for lightning-fast vector similarity\n",
    "- ‚úÖ Tested semantic search that finds movies by meaning\n",
    "- ‚úÖ Explored AI SQL functions (if available)\n",
    "- ‚úÖ Verified columnar engine for analytical acceleration\n",
    "\n",
    "### Security Highlight üîê\n",
    "\n",
    "Notice how we never handled a database password? That's **IAM authentication** in action:\n",
    "- Your Google Cloud identity IS your database identity\n",
    "- The Python Connector handles secure token exchange automatically\n",
    "- No credentials to rotate, leak, or accidentally commit to Git\n",
    "\n",
    "This is the **production-ready** way to handle database authentication in Google Cloud.\n",
    "\n",
    "---\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "Return to the lab instructions for **Task 4**, where you'll build the CymbalFlix Discover web application using Streamlit. You'll create a user interface that lets anyone search for movies semantically and explore AI-powered recommendations!\n",
    "\n",
    "üé¨ Your database is ready to power an AI-driven movie discovery experience! ü§ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: Close the connector when done\n",
    "# Uncomment the line below when you're finished with the notebook\n",
    "# connector.close()\n",
    "# print(\"‚úÖ Connector closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
