{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tTfQV6imPoz"
      },
      "source": [
        "# üé¨ CymbalFlix Discover - Database Setup\n",
        "\n",
        "Welcome to the data engineering portion of CymbalFlix Discover! In this notebook, you'll set up your AlloyDB database with everything needed for an AI-powered movie discovery application.\n",
        "\n",
        "## What We're Building\n",
        "\n",
        "By the end of this notebook, your database will contain:\n",
        "\n",
        "| Table | Records | Purpose |\n",
        "|-------|---------|--------|\n",
        "| `movies` | ~9,700 | Core catalog with AI-searchable summaries and vector embeddings |\n",
        "| `genres` | 20 | Genre lookup table |\n",
        "| `movie_genres` | ~21,000 | Many-to-many junction for movie genres |\n",
        "| `users` | 610 | User profiles extracted from ratings data |\n",
        "| `ratings` | 100,836 | Historical ratings for analytics |\n",
        "| `tags` | 3,683 | User-generated tags for semantic analysis |\n",
        "| `links` | ~9,700 | External IDs (IMDb, TMDb) for integration |\n",
        "| `watchlist` | 0 | Ready for user watchlist operations |\n",
        "\n",
        "## AlloyDB Extensions We'll Enable\n",
        "\n",
        "- **`vector`** - PostgreSQL vector data type for embeddings\n",
        "- **`alloydb_scann`** - Google's ScaNN index for lightning-fast vector search\n",
        "- **`google_ml_integration`** - Direct Vertex AI access from SQL\n",
        "\n",
        "## Security: IAM Authentication\n",
        "\n",
        "Notice something missing? **No database passwords!** We're using IAM authentication, which means:\n",
        "- Your Google Cloud identity is your database identity\n",
        "- No passwords to manage, rotate, or accidentally commit to Git\n",
        "- The AlloyDB Python Connector handles secure authentication automatically\n",
        "\n",
        "Let's get started! üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouE3bt63mPo1"
      },
      "source": [
        "---\n",
        "## Step 1: Configure Your Environment\n",
        "\n",
        "First, let's set up the configuration for your specific AlloyDB cluster. Fill in the form fields below with values from your lab instructions.\n",
        "\n",
        "**Tip:** The form fields appear when you click on this cell. Just fill them in and run the cell!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNNTD9kJmPo1"
      },
      "outputs": [],
      "source": [
        "# @title Configuration - Fill in your lab details { display-mode: \"form\" }\n",
        "# @markdown Enter your project and cluster information from the lab instructions:\n",
        "\n",
        "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "USER_EMAIL = \"\"  # @param {type:\"string\"}\n",
        "CLUSTER_ID = \"cymbalflix-cluster\"  # @param {type:\"string\"}\n",
        "INSTANCE_ID = \"cymbalflix-primary\"  # @param {type:\"string\"}\n",
        "\n",
        "# Database name we'll create\n",
        "DB_NAME = \"cymbalflix\"\n",
        "\n",
        "# GCS bucket with our MovieLens data\n",
        "DATA_BUCKET = \"gs://class-demo/ml-latest-small\"\n",
        "\n",
        "# Validate configuration\n",
        "if not PROJECT_ID or PROJECT_ID == \"\":\n",
        "    print(\"‚ùå Please enter your PROJECT_ID in the form field above!\")\n",
        "    print(\"   You can find it in the lab instructions or Cloud Console.\")\n",
        "else:\n",
        "    print(f\"‚úÖ Configuration set!\")\n",
        "    print(f\"   Project:  {PROJECT_ID}\")\n",
        "    print(f\"   Region:   {REGION}\")\n",
        "    print(f\"   Cluster:  {CLUSTER_ID}\")\n",
        "    print(f\"   Instance: {INSTANCE_ID}\")\n",
        "    print(f\"\\nüîê Using IAM authentication (no password required!)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4nLMp8mmPo2"
      },
      "source": [
        "---\n",
        "## Step 2: Install Dependencies & Connect to AlloyDB\n",
        "\n",
        "We'll use the **AlloyDB Python Connector** with **pg8000** to establish a secure connection. This connector:\n",
        "\n",
        "- Handles IAM authentication automatically\n",
        "- Creates encrypted connections without manual certificate management  \n",
        "- Works seamlessly in Colab, Cloud Shell, or any Python environment\n",
        "- Is the recommended approach for production applications\n",
        "\n",
        "We'll use batch loading with `executemany()` for fast bulk data insertion‚Äîmuch faster than row-by-row inserts!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLtUoWoPmPo2"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q google-cloud-alloydb-connector[pg8000] \\\n",
        "    pandas google-cloud-storage sqlalchemy\n",
        "\n",
        "print(\"‚úÖ Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHNc9KnbmPo2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.cloud import storage\n",
        "from google.cloud.alloydb.connector import Connector, IPTypes\n",
        "import pg8000\n",
        "import sqlalchemy\n",
        "from sqlalchemy import text\n",
        "import io\n",
        "import re\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Build the instance URI for the connector\n",
        "INSTANCE_URI = f\"projects/{PROJECT_ID}/locations/{REGION}/clusters/{CLUSTER_ID}/instances/{INSTANCE_ID}\"\n",
        "\n",
        "# Initialize the AlloyDB connector\n",
        "connector = Connector()\n",
        "\n",
        "def get_connection(database=\"postgres\"):\n",
        "    \"\"\"\n",
        "    Create a connection to AlloyDB using the Python Connector.\n",
        "\n",
        "    With enable_iam_auth=True, your Google Cloud identity is used\n",
        "    for authentication - no password needed!\n",
        "    \"\"\"\n",
        "    conn = connector.connect(\n",
        "        INSTANCE_URI,\n",
        "        \"pg8000\",\n",
        "        user=USER_EMAIL,\n",
        "        db=database,\n",
        "        enable_iam_auth=True,\n",
        "        ip_type=IPTypes.PUBLIC,\n",
        "    )\n",
        "    return conn\n",
        "\n",
        "# Test the connection\n",
        "print(f\"üîó Connecting to: {INSTANCE_URI}\")\n",
        "print(\"‚è≥ Establishing secure connection...\")\n",
        "\n",
        "try:\n",
        "    conn = get_connection()\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(\"SELECT version();\")\n",
        "    version = cursor.fetchone()[0]\n",
        "    cursor.execute(\"SELECT current_user;\")\n",
        "    current_user = cursor.fetchone()[0]\n",
        "    cursor.close()\n",
        "    conn.close()\n",
        "\n",
        "    print(\"\\n‚úÖ Successfully connected to AlloyDB!\")\n",
        "    print(f\"\\nüîê Authenticated as: {current_user}\")\n",
        "    print(f\"\\nüìä Database version:\")\n",
        "    print(f\"   {version[:60]}...\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Connection failed: {e}\")\n",
        "    print(\"\\nüîç Troubleshooting tips:\")\n",
        "    print(\"   1. Verify your PROJECT_ID is correct (check the form above)\")\n",
        "    print(\"   2. Make sure your AlloyDB cluster shows 'Ready' in Cloud Console\")\n",
        "    print(\"   3. Confirm the cluster and instance names match your Terraform output\")\n",
        "    print(\"   4. Check that your user has the AlloyDB IAM Database User role\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwcKUG6dmPo2"
      },
      "source": [
        "---\n",
        "## Step 3: Create the CymbalFlix Database\n",
        "\n",
        "We'll create a dedicated database for CymbalFlix rather than using the default `postgres` database. This is a best practice‚Äîit keeps your application data isolated and makes it easier to manage permissions, backups, and migrations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmaDinxvmPo2"
      },
      "outputs": [],
      "source": [
        "# Create the cymbalflix database\n",
        "# We need to use autocommit mode for CREATE DATABASE\n",
        "conn = get_connection(\"postgres\")\n",
        "conn.autocommit = True\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Check if database exists\n",
        "cursor.execute(\"SELECT 1 FROM pg_database WHERE datname = %s\", (DB_NAME,))\n",
        "exists = cursor.fetchone()\n",
        "\n",
        "if not exists:\n",
        "    cursor.execute(f\"CREATE DATABASE {DB_NAME}\")\n",
        "    print(f\"‚úÖ Created database: {DB_NAME}\")\n",
        "else:\n",
        "    print(f\"‚ÑπÔ∏è  Database '{DB_NAME}' already exists - continuing...\")\n",
        "\n",
        "cursor.close()\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjSIA1-omPo2"
      },
      "source": [
        "---\n",
        "## Step 4: Enable Extensions\n",
        "\n",
        "This is where AlloyDB becomes more than just PostgreSQL! We'll enable three powerful extensions:\n",
        "\n",
        "| Extension | What It Does |\n",
        "|-----------|-------------|\n",
        "| `vector` | Adds the VECTOR data type for storing embeddings |\n",
        "| `alloydb_scann` | Enables Google's ScaNN algorithm for fast similarity search |\n",
        "| `google_ml_integration` | Connects AlloyDB directly to Vertex AI |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVYeMUpXmPo3"
      },
      "outputs": [],
      "source": [
        "# Enable AlloyDB extensions\n",
        "conn = get_connection(DB_NAME)\n",
        "conn.autocommit = True\n",
        "cursor = conn.cursor()\n",
        "\n",
        "extensions = [\n",
        "    (\"vector\", \"Vector data type for embeddings\"),\n",
        "    (\"alloydb_scann\", \"ScaNN index for lightning-fast vector similarity search\"),\n",
        "    (\"google_ml_integration\", \"Direct Vertex AI integration for AI SQL functions\")\n",
        "]\n",
        "\n",
        "print(\"üîß Enabling AlloyDB extensions...\\n\")\n",
        "\n",
        "for ext_name, description in extensions:\n",
        "    try:\n",
        "        cursor.execute(f\"CREATE EXTENSION IF NOT EXISTS {ext_name}\")\n",
        "        print(f\"‚úÖ {ext_name}\")\n",
        "        print(f\"   ‚îî‚îÄ {description}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Could not enable {ext_name}: {e}\")\n",
        "\n",
        "cursor.close()\n",
        "conn.close()\n",
        "\n",
        "print(\"\\nüéâ Extensions enabled!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHn1la_0mPo3"
      },
      "source": [
        "---\n",
        "## Step 5: Create the Database Schema\n",
        "\n",
        "Our schema is designed for both transactional operations (watchlists, ratings) and analytical queries (trending movies, genre analysis).\n",
        "\n",
        "**Key design decisions:**\n",
        "\n",
        "- **Normalized genres** - Instead of storing \"Action|Comedy|Sci-Fi\" as text, we use a proper junction table\n",
        "- **Vector column** - The `movies.summary_embedding` stores 3072-dimensional vectors for semantic search\n",
        "- **Foreign keys** - Enforce data integrity across related tables\n",
        "- **Timestamps** - Enable temporal analysis and audit trails\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ   movies    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ movie_genres ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ   genres    ‚îÇ\n",
        "‚îÇ (+ vector)  ‚îÇ       ‚îÇ  (junction)  ‚îÇ       ‚îÇ  (lookup)   ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "       ‚îÇ\n",
        "       ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "       ‚îÇ                    ‚îÇ                     ‚îÇ\n",
        "       ‚ñº                    ‚ñº                     ‚ñº\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ   ratings   ‚îÇ       ‚îÇ    tags     ‚îÇ       ‚îÇ   links     ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "       ‚îÇ                    ‚îÇ\n",
        "       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                ‚ñº\n",
        "          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "          ‚îÇ    users    ‚îÇ\n",
        "          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                ‚îÇ\n",
        "                ‚ñº\n",
        "          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "          ‚îÇ  watchlist  ‚îÇ\n",
        "          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8i_K9GXmPo3"
      },
      "outputs": [],
      "source": [
        "# Define our database schema\n",
        "schema_sql = \"\"\"\n",
        "-- Core movie catalog with vector embeddings for semantic search\n",
        "CREATE TABLE IF NOT EXISTS movies (\n",
        "    movie_id INTEGER PRIMARY KEY,\n",
        "    title VARCHAR(255) NOT NULL,\n",
        "    year INTEGER,\n",
        "    summary TEXT,\n",
        "    summary_embedding VECTOR(3072)\n",
        ");\n",
        "\n",
        "-- Genre lookup table\n",
        "CREATE TABLE IF NOT EXISTS genres (\n",
        "    genre_id SERIAL PRIMARY KEY,\n",
        "    genre_name VARCHAR(50) UNIQUE NOT NULL\n",
        ");\n",
        "\n",
        "-- Many-to-many junction table for movie genres\n",
        "CREATE TABLE IF NOT EXISTS movie_genres (\n",
        "    movie_id INTEGER REFERENCES movies(movie_id) ON DELETE CASCADE,\n",
        "    genre_id INTEGER REFERENCES genres(genre_id) ON DELETE CASCADE,\n",
        "    PRIMARY KEY (movie_id, genre_id)\n",
        ");\n",
        "\n",
        "-- User profiles (extracted from ratings data)\n",
        "CREATE TABLE IF NOT EXISTS users (\n",
        "    user_id INTEGER PRIMARY KEY,\n",
        "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        ");\n",
        "\n",
        "-- Historical ratings for analytics\n",
        "CREATE TABLE IF NOT EXISTS ratings (\n",
        "    rating_id SERIAL PRIMARY KEY,\n",
        "    user_id INTEGER REFERENCES users(user_id) ON DELETE CASCADE,\n",
        "    movie_id INTEGER REFERENCES movies(movie_id) ON DELETE CASCADE,\n",
        "    rating NUMERIC(2,1) NOT NULL CHECK (rating >= 0.5 AND rating <= 5.0),\n",
        "    rated_at TIMESTAMP\n",
        ");\n",
        "\n",
        "-- User-generated tags for semantic analysis\n",
        "CREATE TABLE IF NOT EXISTS tags (\n",
        "    tag_id SERIAL PRIMARY KEY,\n",
        "    user_id INTEGER REFERENCES users(user_id) ON DELETE CASCADE,\n",
        "    movie_id INTEGER REFERENCES movies(movie_id) ON DELETE CASCADE,\n",
        "    tag_text VARCHAR(255) NOT NULL,\n",
        "    tagged_at TIMESTAMP\n",
        ");\n",
        "\n",
        "-- User watchlists (for transactional operations)\n",
        "CREATE TABLE IF NOT EXISTS watchlist (\n",
        "    user_id INTEGER REFERENCES users(user_id) ON DELETE CASCADE,\n",
        "    movie_id INTEGER REFERENCES movies(movie_id) ON DELETE CASCADE,\n",
        "    added_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "    PRIMARY KEY (user_id, movie_id)\n",
        ");\n",
        "\n",
        "-- External database links (IMDb, TMDb)\n",
        "CREATE TABLE IF NOT EXISTS links (\n",
        "    movie_id INTEGER PRIMARY KEY REFERENCES movies(movie_id) ON DELETE CASCADE,\n",
        "    imdb_id VARCHAR(20),\n",
        "    tmdb_id INTEGER\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "# Execute the schema\n",
        "conn = get_connection(DB_NAME)\n",
        "cursor = conn.cursor()\n",
        "cursor.execute(schema_sql)\n",
        "conn.commit()\n",
        "cursor.close()\n",
        "conn.close()\n",
        "\n",
        "print(\"‚úÖ Database schema created!\")\n",
        "print(\"\\nüìã Tables created:\")\n",
        "print(\"   ‚Ä¢ movies (with VECTOR(3072) for embeddings)\")\n",
        "print(\"   ‚Ä¢ genres\")\n",
        "print(\"   ‚Ä¢ movie_genres (junction table)\")\n",
        "print(\"   ‚Ä¢ users\")\n",
        "print(\"   ‚Ä¢ ratings\")\n",
        "print(\"   ‚Ä¢ tags\")\n",
        "print(\"   ‚Ä¢ watchlist\")\n",
        "print(\"   ‚Ä¢ links (IMDb/TMDb IDs)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqMJoLqjJIfi"
      },
      "outputs": [],
      "source": [
        "# Grant permissions to the application service account (for Cloud Run deployment)\n",
        "app_sa_db_user = f\"cymbalflix-app@{PROJECT_ID}.iam\"\n",
        "\n",
        "grant_sql = f'''\n",
        "GRANT USAGE ON SCHEMA public TO \"{app_sa_db_user}\";\n",
        "GRANT SELECT, INSERT, UPDATE ON ALL TABLES IN SCHEMA public TO \"{app_sa_db_user}\";\n",
        "'''\n",
        "\n",
        "conn = get_connection(DB_NAME)\n",
        "conn.autocommit = True\n",
        "cursor = conn.cursor()\n",
        "cursor.execute(grant_sql)\n",
        "cursor.close()\n",
        "conn.close()\n",
        "\n",
        "print(f\"‚úÖ Granted database permissions to {app_sa_db_user}\")\n",
        "print(\"   (This enables Cloud Run deployment later)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQrOYnPwlyik"
      },
      "source": [
        "---\n",
        "## Step 6: Load Data via GCS Import\n",
        "\n",
        "Now comes the fun part‚Äîloading our MovieLens data! We'll use **AlloyDB's native GCS import** for blazing-fast bulk loading:\n",
        "\n",
        "1. **Transform** - Clean and prepare data in pandas DataFrames\n",
        "2. **Stage** - Upload transformed CSVs to a GCS bucket\n",
        "3. **Import** - Use `gcloud alloydb clusters import` for server-side loading\n",
        "\n",
        "**Why GCS import?** It's the fastest way to load data into AlloyDB because:\n",
        "- Data flows directly from GCS to AlloyDB (no client bottleneck)\n",
        "- Uses optimized server-side COPY operations\n",
        "- Can load 100K+ rows in **seconds** instead of minutes\n",
        "\n",
        "Let's start by setting up our staging bucket and helper functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHPbI2nulyik"
      },
      "outputs": [],
      "source": [
        "# Create a staging bucket for our transformed data\n",
        "STAGING_BUCKET = PROJECT_ID  # Use project ID as bucket name (guaranteed unique)\n",
        "\n",
        "# Get project number for service account\n",
        "import subprocess\n",
        "result = subprocess.run(\n",
        "    ['gcloud', 'projects', 'describe', PROJECT_ID, '--format=value(projectNumber)'],\n",
        "    capture_output=True, text=True\n",
        ")\n",
        "PROJECT_NUMBER = result.stdout.strip()\n",
        "ALLOYDB_SA = f\"service-{PROJECT_NUMBER}@gcp-sa-alloydb.iam.gserviceaccount.com\"\n",
        "\n",
        "print(f\"üì¶ Staging bucket: gs://{STAGING_BUCKET}\")\n",
        "print(f\"üîê AlloyDB service account: {ALLOYDB_SA}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7M498wCOlyik"
      },
      "outputs": [],
      "source": [
        "%%bash -s \"$STAGING_BUCKET\" \"$ALLOYDB_SA\" \"$REGION\"\n",
        "BUCKET=$1\n",
        "SA=$2\n",
        "REGION=$3\n",
        "\n",
        "# Create bucket if it doesn't exist\n",
        "if ! gcloud storage buckets describe gs://$BUCKET &>/dev/null; then\n",
        "    echo \"üì¶ Creating staging bucket gs://$BUCKET...\"\n",
        "    gcloud storage buckets create gs://$BUCKET --location=$REGION\n",
        "else\n",
        "    echo \"üì¶ Bucket gs://$BUCKET already exists\"\n",
        "fi\n",
        "\n",
        "# Grant AlloyDB service account read access\n",
        "echo \"üîê Granting AlloyDB service account access...\"\n",
        "gcloud storage buckets add-iam-policy-binding gs://$BUCKET \\\n",
        "    --member=\"serviceAccount:$SA\" \\\n",
        "    --role=\"roles/storage.objectViewer\" \\\n",
        "    --quiet\n",
        "\n",
        "echo \"‚úÖ Bucket ready for staging!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H3Ar_03lyik"
      },
      "outputs": [],
      "source": [
        "def load_csv_from_gcs(bucket_path, filename):\n",
        "    \"\"\"Load a CSV file from GCS into a pandas DataFrame.\"\"\"\n",
        "    path = bucket_path\n",
        "    if path.startswith(\"gs://\"):\n",
        "        path = path[5:]\n",
        "\n",
        "    if \"/\" in path:\n",
        "        parts = path.split(\"/\", 1)\n",
        "        bucket_name = parts[0]\n",
        "        blob_path = f\"{parts[1]}/{filename}\"\n",
        "    else:\n",
        "        bucket_name = path\n",
        "        blob_path = filename\n",
        "\n",
        "    client = storage.Client(project=PROJECT_ID)\n",
        "    bucket = client.bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_path)\n",
        "\n",
        "    content = blob.download_as_text()\n",
        "    return pd.read_csv(io.StringIO(content))\n",
        "\n",
        "\n",
        "def upload_csv_to_gcs(df, filename, columns=None):\n",
        "    \"\"\"Upload a DataFrame as CSV to our staging bucket.\"\"\"\n",
        "    client = storage.Client(project=PROJECT_ID)\n",
        "    bucket = client.bucket(STAGING_BUCKET)\n",
        "    blob = bucket.blob(f\"cymbalflix/{filename}\")\n",
        "\n",
        "    # Select columns if specified\n",
        "    if columns:\n",
        "        df = df[columns]\n",
        "\n",
        "    # Convert to CSV without index\n",
        "    csv_data = df.to_csv(index=False, header=False)\n",
        "    blob.upload_from_string(csv_data, content_type='text/csv')\n",
        "\n",
        "    return f\"gs://{STAGING_BUCKET}/cymbalflix/{filename}\"\n",
        "\n",
        "\n",
        "def import_csv_to_alloydb(gcs_uri, table_name, columns):\n",
        "    \"\"\"Import a CSV from GCS into AlloyDB using gcloud.\"\"\"\n",
        "    import subprocess\n",
        "\n",
        "    # Build column list for --columns flag\n",
        "    columns_str = ','.join(columns)\n",
        "\n",
        "    cmd = [\n",
        "        'gcloud', 'alloydb', 'clusters', 'import',\n",
        "        CLUSTER_ID,\n",
        "        f'--region={REGION}',\n",
        "        f'--gcs-uri={gcs_uri}',\n",
        "        f'--database={DB_NAME}',\n",
        "        f'--user={USER_EMAIL}',\n",
        "        '--csv',\n",
        "        f'--table={table_name}',\n",
        "        f'--columns={columns_str}',\n",
        "        '--quiet'\n",
        "    ]\n",
        "\n",
        "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "\n",
        "    if result.returncode != 0:\n",
        "        print(f\"‚ùå Error importing {table_name}: {result.stderr}\")\n",
        "        raise Exception(result.stderr)\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "print(\"‚úÖ Helper functions ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BieJffzclyik"
      },
      "source": [
        "### 6.1 Load and Transform Movies\n",
        "\n",
        "The MovieLens dataset stores the year in the title (e.g., \"Jumanji (1995)\"). We'll extract it into a separate column for better querying and analytics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFzbHQmBlyik"
      },
      "outputs": [],
      "source": [
        "# Load movies from GCS\n",
        "print(\"üì• Loading movies.csv from GCS...\")\n",
        "movies_df = load_csv_from_gcs(DATA_BUCKET, \"movies.csv\")\n",
        "print(f\"   Loaded {len(movies_df):,} movies\")\n",
        "\n",
        "# Extract year from title using regex\n",
        "def extract_year_and_clean_title(title):\n",
        "    match = re.search(r'\\s*\\((\\d{4})\\)\\s*$', str(title))\n",
        "    if match:\n",
        "        year = int(match.group(1))\n",
        "        clean_title = re.sub(r'\\s*\\(\\d{4}\\)\\s*$', '', title).strip()\n",
        "        return clean_title, year\n",
        "    return title, None\n",
        "\n",
        "movies_df[['clean_title', 'year']] = movies_df['title'].apply(\n",
        "    lambda x: pd.Series(extract_year_and_clean_title(x))\n",
        ")\n",
        "movies_df['title'] = movies_df['clean_title']\n",
        "movies_df = movies_df.drop(columns=['clean_title'])\n",
        "\n",
        "# Store genres for later processing\n",
        "movies_with_genres = movies_df[['movieId', 'genres']].copy()\n",
        "\n",
        "print(\"\\n‚úÖ Movies processed!\")\n",
        "display(movies_df[['movieId', 'title', 'year']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr4aiaW4lyik"
      },
      "source": [
        "### 6.2 Load and Merge Summaries\n",
        "\n",
        "The summaries were generated using Gemini to provide rich, searchable descriptions of each movie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQGuoho0lyik"
      },
      "outputs": [],
      "source": [
        "# Load summaries\n",
        "print(\"üì• Loading summaries.csv from GCS...\")\n",
        "summaries_df = load_csv_from_gcs(DATA_BUCKET, \"summaries.csv\")\n",
        "print(f\"   Loaded {len(summaries_df):,} summaries\")\n",
        "\n",
        "# Merge summaries into movies\n",
        "movies_df = movies_df.merge(summaries_df, on='movieId', how='left')\n",
        "\n",
        "print(\"\\n‚úÖ Summaries merged!\")\n",
        "\n",
        "sample_movie = movies_df.iloc[0]\n",
        "if pd.notna(sample_movie.get('summary')):\n",
        "    print(f\"\\nüìù Sample summary for '{sample_movie['title']}':\")\n",
        "    print(f\"   {sample_movie['summary'][:250]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnFl1iLQlyik"
      },
      "source": [
        "### 6.3 Load and Merge Embeddings\n",
        "\n",
        "The embeddings are 3072-dimensional vectors from Gemini's embedding model, enabling semantic similarity search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiy6m0pjlyik"
      },
      "outputs": [],
      "source": [
        "# Load embeddings\n",
        "print(\"üì• Loading embeddings.csv from GCS...\")\n",
        "embeddings_df = load_csv_from_gcs(DATA_BUCKET, \"embeddings.csv\")\n",
        "print(f\"   Loaded {len(embeddings_df):,} embeddings\")\n",
        "\n",
        "# Merge embeddings into movies\n",
        "movies_df = movies_df.merge(embeddings_df, on='movieId', how='left')\n",
        "\n",
        "sample_embedding = movies_df.iloc[0].get('embedding')\n",
        "if pd.notna(sample_embedding):\n",
        "    try:\n",
        "        embedding_values = json.loads(sample_embedding)\n",
        "        print(f\"\\n‚úÖ Embeddings merged!\")\n",
        "        print(f\"   Dimensions: {len(embedding_values)}\")\n",
        "    except:\n",
        "        print(\"\\n‚úÖ Embeddings merged!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4EVu47vlyik"
      },
      "source": [
        "### 6.4 Prepare All Data for Import\n",
        "\n",
        "Now we'll load the remaining data (ratings, tags, links) and prepare all DataFrames for CSV export."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEaTEnX6lyik"
      },
      "outputs": [],
      "source": [
        "# Load ratings\n",
        "print(\"üì• Loading ratings.csv from GCS...\")\n",
        "ratings_df = load_csv_from_gcs(DATA_BUCKET, \"ratings.csv\")\n",
        "print(f\"   Loaded {len(ratings_df):,} ratings\")\n",
        "\n",
        "# Load tags\n",
        "print(\"üì• Loading tags.csv from GCS...\")\n",
        "tags_df = load_csv_from_gcs(DATA_BUCKET, \"tags.csv\")\n",
        "print(f\"   Loaded {len(tags_df):,} tags\")\n",
        "\n",
        "# Load links\n",
        "print(\"üì• Loading links.csv from GCS...\")\n",
        "links_df = load_csv_from_gcs(DATA_BUCKET, \"links.csv\")\n",
        "print(f\"   Loaded {len(links_df):,} links\")\n",
        "\n",
        "print(\"\\n‚úÖ All source data loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8UNhzy1lyik"
      },
      "outputs": [],
      "source": [
        "# Prepare movies DataFrame\n",
        "movies_load_df = movies_df[['movieId', 'title', 'year', 'summary', 'embedding']].copy()\n",
        "movies_load_df.columns = ['movie_id', 'title', 'year', 'summary', 'summary_embedding']\n",
        "# Handle NaN years - use empty string for CSV\n",
        "movies_load_df['year'] = movies_load_df['year'].apply(\n",
        "    lambda x: int(x) if pd.notna(x) else ''\n",
        ")\n",
        "\n",
        "# Prepare genres DataFrame\n",
        "all_genres = set()\n",
        "for genres_str in movies_with_genres['genres']:\n",
        "    if pd.notna(genres_str) and genres_str != '(no genres listed)':\n",
        "        all_genres.update(genres_str.split('|'))\n",
        "genres_load_df = pd.DataFrame({'genre_name': sorted(all_genres)})\n",
        "genre_lookup = {name: idx + 1 for idx, name in enumerate(sorted(all_genres))}\n",
        "\n",
        "# Prepare movie_genres junction table\n",
        "junction_records = []\n",
        "for _, row in movies_with_genres.iterrows():\n",
        "    if pd.notna(row['genres']) and row['genres'] != '(no genres listed)':\n",
        "        movie_id = int(row['movieId'])\n",
        "        for genre in row['genres'].split('|'):\n",
        "            if genre in genre_lookup:\n",
        "                junction_records.append({\n",
        "                    'movie_id': movie_id,\n",
        "                    'genre_id': genre_lookup[genre]\n",
        "                })\n",
        "movie_genres_load_df = pd.DataFrame(junction_records)\n",
        "\n",
        "# Prepare users DataFrame\n",
        "users_load_df = pd.DataFrame({'user_id': sorted(ratings_df['userId'].unique())})\n",
        "\n",
        "# Prepare ratings DataFrame\n",
        "ratings_load_df = ratings_df[['userId', 'movieId', 'rating', 'timestamp']].copy()\n",
        "ratings_load_df.columns = ['user_id', 'movie_id', 'rating', 'rated_at']\n",
        "ratings_load_df['rated_at'] = pd.to_datetime(ratings_load_df['rated_at'], unit='s')\n",
        "\n",
        "# Prepare tags DataFrame\n",
        "tags_load_df = tags_df[['userId', 'movieId', 'tag', 'timestamp']].copy()\n",
        "tags_load_df.columns = ['user_id', 'movie_id', 'tag_text', 'tagged_at']\n",
        "tags_load_df['tagged_at'] = pd.to_datetime(tags_load_df['tagged_at'], unit='s')\n",
        "\n",
        "# Prepare links DataFrame\n",
        "links_load_df = links_df[['movieId', 'imdbId', 'tmdbId']].copy()\n",
        "links_load_df.columns = ['movie_id', 'imdb_id', 'tmdb_id']\n",
        "links_load_df['imdb_id'] = links_load_df['imdb_id'].apply(\n",
        "    lambda x: f\"tt{int(x):07d}\" if pd.notna(x) else ''\n",
        ")\n",
        "links_load_df['tmdb_id'] = links_load_df['tmdb_id'].apply(\n",
        "    lambda x: int(x) if pd.notna(x) else ''\n",
        ")\n",
        "\n",
        "print(\"üìä Prepared DataFrames:\")\n",
        "print(f\"   movies:       {len(movies_load_df):>8,} rows\")\n",
        "print(f\"   genres:       {len(genres_load_df):>8,} rows\")\n",
        "print(f\"   movie_genres: {len(movie_genres_load_df):>8,} rows\")\n",
        "print(f\"   users:        {len(users_load_df):>8,} rows\")\n",
        "print(f\"   ratings:      {len(ratings_load_df):>8,} rows\")\n",
        "print(f\"   tags:         {len(tags_load_df):>8,} rows\")\n",
        "print(f\"   links:        {len(links_load_df):>8,} rows\")\n",
        "print(\"\\n‚úÖ All data prepared for import!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Pn1OWo-lyik"
      },
      "source": [
        "### 6.5 Upload to Staging Bucket\n",
        "\n",
        "Now we'll upload all the prepared CSVs to our staging bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdvtldQXlyik"
      },
      "outputs": [],
      "source": [
        "print(\"üì§ Uploading transformed data to GCS...\\n\")\n",
        "\n",
        "# Upload each DataFrame\n",
        "uploads = [\n",
        "    ('movies.csv', movies_load_df, ['movie_id', 'title', 'year', 'summary', 'summary_embedding']),\n",
        "    ('genres.csv', genres_load_df, ['genre_name']),\n",
        "    ('movie_genres.csv', movie_genres_load_df, ['movie_id', 'genre_id']),\n",
        "    ('users.csv', users_load_df, ['user_id']),\n",
        "    ('ratings.csv', ratings_load_df, ['user_id', 'movie_id', 'rating', 'rated_at']),\n",
        "    ('tags.csv', tags_load_df, ['user_id', 'movie_id', 'tag_text', 'tagged_at']),\n",
        "    ('links.csv', links_load_df, ['movie_id', 'imdb_id', 'tmdb_id']),\n",
        "]\n",
        "\n",
        "gcs_uris = {}\n",
        "for filename, df, columns in uploads:\n",
        "    uri = upload_csv_to_gcs(df, filename, columns)\n",
        "    table_name = filename.replace('.csv', '')\n",
        "    gcs_uris[table_name] = (uri, columns)\n",
        "    print(f\"   ‚úÖ {filename}: {len(df):,} rows\")\n",
        "\n",
        "print(f\"\\nüì¶ All files staged at gs://{STAGING_BUCKET}/cymbalflix/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Py6fFKglyik"
      },
      "source": [
        "### 6.6 Import Data into AlloyDB\n",
        "\n",
        "Now for the fast part! We'll use `gcloud alloydb clusters import` to load data directly from GCS into AlloyDB. This is **much faster** than client-side inserts because:\n",
        "\n",
        "- Data flows directly from GCS to AlloyDB\n",
        "- No network bottleneck through our notebook\n",
        "- Uses optimized server-side COPY operations\n",
        "\n",
        "‚è±Ô∏è **Expected time:** ~90 seconds for all 7 tables (vs 13+ minutes with client inserts!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0uQXomdlyil"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Import order matters due to foreign key constraints\n",
        "# Tables with no dependencies first, then dependent tables\n",
        "import_order = [\n",
        "    ('movies', ['movie_id', 'title', 'year', 'summary', 'summary_embedding']),\n",
        "    ('genres', ['genre_name']),\n",
        "    ('movie_genres', ['movie_id', 'genre_id']),\n",
        "    ('users', ['user_id']),\n",
        "    ('ratings', ['user_id', 'movie_id', 'rating', 'rated_at']),\n",
        "    ('tags', ['user_id', 'movie_id', 'tag_text', 'tagged_at']),\n",
        "    ('links', ['movie_id', 'imdb_id', 'tmdb_id']),\n",
        "]\n",
        "\n",
        "print(\"üöÄ Importing data into AlloyDB...\\n\")\n",
        "total_start = time.time()\n",
        "\n",
        "for table_name, columns in import_order:\n",
        "    uri = f\"gs://{STAGING_BUCKET}/cymbalflix/{table_name}.csv\"\n",
        "    print(f\"   üì• Importing {table_name}...\", end=\" \", flush=True)\n",
        "\n",
        "    start = time.time()\n",
        "    import_csv_to_alloydb(uri, table_name, columns)\n",
        "    elapsed = time.time() - start\n",
        "\n",
        "    print(f\"‚úÖ ({elapsed:.1f}s)\")\n",
        "\n",
        "total_elapsed = time.time() - total_start\n",
        "print(f\"\\nüéâ All data imported in {total_elapsed:.1f} seconds!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGaf_7ovmPo4"
      },
      "source": [
        "---\n",
        "## Step 7: Verify Your Data\n",
        "\n",
        "Let's make sure everything loaded correctly with some verification queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mv9ShzvomPo4"
      },
      "outputs": [],
      "source": [
        "# Verification queries\n",
        "conn = get_connection(DB_NAME)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "verification_queries = [\n",
        "    (\"movies\", \"SELECT COUNT(*) FROM movies\"),\n",
        "    (\"  ‚îî‚îÄ with summaries\", \"SELECT COUNT(*) FROM movies WHERE summary IS NOT NULL\"),\n",
        "    (\"  ‚îî‚îÄ with embeddings\", \"SELECT COUNT(*) FROM movies WHERE summary_embedding IS NOT NULL\"),\n",
        "    (\"genres\", \"SELECT COUNT(*) FROM genres\"),\n",
        "    (\"movie_genres\", \"SELECT COUNT(*) FROM movie_genres\"),\n",
        "    (\"users\", \"SELECT COUNT(*) FROM users\"),\n",
        "    (\"ratings\", \"SELECT COUNT(*) FROM ratings\"),\n",
        "    (\"tags\", \"SELECT COUNT(*) FROM tags\"),\n",
        "    (\"links\", \"SELECT COUNT(*) FROM links\"),\n",
        "]\n",
        "\n",
        "print(\"üìä Data Verification Report\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "for name, query in verification_queries:\n",
        "    cursor.execute(query)\n",
        "    count = cursor.fetchone()[0]\n",
        "    print(f\"   {name}: {count:,}\")\n",
        "\n",
        "cursor.close()\n",
        "conn.close()\n",
        "\n",
        "print(\"=\" * 45)\n",
        "print(\"\\n‚úÖ All data loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t90hjYt9mPo4"
      },
      "outputs": [],
      "source": [
        "# Sample query: Top-rated movies with their genres\n",
        "sample_query = \"\"\"\n",
        "SELECT\n",
        "    m.title,\n",
        "    m.year,\n",
        "    ROUND(AVG(r.rating)::numeric, 2) as avg_rating,\n",
        "    COUNT(r.rating_id) as num_ratings,\n",
        "    STRING_AGG(DISTINCT g.genre_name, ', ' ORDER BY g.genre_name) as genres\n",
        "FROM movies m\n",
        "JOIN ratings r ON m.movie_id = r.movie_id\n",
        "JOIN movie_genres mg ON m.movie_id = mg.movie_id\n",
        "JOIN genres g ON mg.genre_id = g.genre_id\n",
        "GROUP BY m.movie_id, m.title, m.year\n",
        "HAVING COUNT(r.rating_id) >= 50\n",
        "ORDER BY avg_rating DESC, num_ratings DESC\n",
        "LIMIT 10;\n",
        "\"\"\"\n",
        "\n",
        "conn = get_connection(DB_NAME)\n",
        "result_df = pd.read_sql(sample_query, conn)\n",
        "conn.close()\n",
        "\n",
        "print(\"üèÜ Top 10 Highest-Rated Movies (minimum 50 ratings):\")\n",
        "display(result_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-5VLavrmPo4"
      },
      "source": [
        "---\n",
        "## Step 8: Create the ScaNN Index\n",
        "\n",
        "Now for the feature that makes AlloyDB special for AI workloads‚Äîthe **ScaNN index**.\n",
        "\n",
        "**What is ScaNN?** Scalable Nearest Neighbors is Google's algorithm for fast vector similarity search. It's the same technology that powers Google Search's ability to find similar content across billions of documents.\n",
        "\n",
        "**Why do we need it?** Without an index, finding similar movies requires comparing your query vector against every single movie‚Äîthat's 9,700 comparisons. With ScaNN, the search narrows to a small subset almost instantly.\n",
        "\n",
        "| Without ScaNN | With ScaNN |\n",
        "|--------------|------------|\n",
        "| Compare against all 9,700 movies | Compare against ~50 candidates |\n",
        "| Linear time O(n) | Logarithmic time O(log n) |\n",
        "| ~100ms per query | ~5ms per query |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66R7mIDvmPo4"
      },
      "outputs": [],
      "source": [
        "# Create the ScaNN index\n",
        "conn = get_connection(DB_NAME)\n",
        "conn.autocommit = True\n",
        "cursor = conn.cursor()\n",
        "\n",
        "print(\"üîß Creating ScaNN index on movie embeddings...\")\n",
        "print(\"   This may take a moment...\\n\")\n",
        "\n",
        "try:\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE INDEX IF NOT EXISTS movies_embedding_scann_idx\n",
        "        ON movies USING scann (summary_embedding cosine)\n",
        "        WITH (num_leaves = 50, quantizer = 'sq8');\n",
        "    \"\"\")\n",
        "    print(\"‚úÖ ScaNN index created!\")\n",
        "    print(\"\\nüìä Index configuration:\")\n",
        "    print(\"   ‚Ä¢ Distance metric: cosine (measures angle between vectors)\")\n",
        "    print(\"   ‚Ä¢ num_leaves: 50 (partitions for efficient search)\")\n",
        "    print(\"   ‚Ä¢ quantizer: sq8 (8-bit scalar quantization for speed)\")\n",
        "except Exception as e:\n",
        "    if \"already exists\" in str(e).lower():\n",
        "        print(\"‚ÑπÔ∏è  ScaNN index already exists\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Could not create index: {e}\")\n",
        "\n",
        "cursor.close()\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfTbdp8_mPo4"
      },
      "source": [
        "---\n",
        "## Step 9: Semantic Search Demo üéØ\n",
        "\n",
        "This is the payoff! Let's see semantic search in action.\n",
        "\n",
        "**How it works:**\n",
        "1. Your search query gets converted to a 3072-dimensional vector using Gemini's embedding model\n",
        "2. AlloyDB uses the ScaNN index to find movies with similar vectors\n",
        "3. Results are ranked by cosine similarity (1.0 = identical, 0.0 = completely different)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2f4bLwDmPo7"
      },
      "outputs": [],
      "source": [
        "def semantic_search(query, limit=5):\n",
        "    \"\"\"\n",
        "    Search for movies using semantic similarity.\n",
        "\n",
        "    This converts your natural language query into a vector,\n",
        "    then finds movies with similar vectors.\n",
        "    \"\"\"\n",
        "    conn = get_connection(DB_NAME)\n",
        "\n",
        "    search_sql = \"\"\"\n",
        "    WITH query_embedding AS (\n",
        "    SELECT embedding(\n",
        "        'gemini-embedding-001',   -- no registration needed\n",
        "            %s                        -- the query text from Python\n",
        "        )::vector AS embedding\n",
        "    )\n",
        "    SELECT\n",
        "        m.title,\n",
        "        m.year,\n",
        "        ROUND((1 - (m.summary_embedding <=> q.embedding))::numeric, 3) AS similarity,\n",
        "        LEFT(m.summary, 150) || '...' AS summary_preview\n",
        "    FROM movies m\n",
        "    CROSS JOIN query_embedding q\n",
        "    WHERE m.summary_embedding IS NOT NULL\n",
        "    ORDER BY m.summary_embedding <=> q.embedding\n",
        "    LIMIT %s;\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    result = pd.read_sql(search_sql, conn, params=(query, limit))\n",
        "    conn.close()\n",
        "    return result\n",
        "\n",
        "print(\"‚úÖ Semantic search function ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2un2cpnmPo7"
      },
      "outputs": [],
      "source": [
        "# Demo 1: Conceptual search\n",
        "print(\"üîç Search: 'A movie about artificial intelligence becoming self-aware'\")\n",
        "print(\"=\" * 70)\n",
        "results = semantic_search(\"A movie about artificial intelligence becoming self-aware\")\n",
        "display(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgmKeErImPo7"
      },
      "outputs": [],
      "source": [
        "# Demo 2: Emotional/thematic search\n",
        "print(\"üîç Search: 'Heartwarming story about unlikely friendship'\")\n",
        "print(\"=\" * 70)\n",
        "results = semantic_search(\"Heartwarming story about unlikely friendship\")\n",
        "display(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCueb0ixmPo7"
      },
      "outputs": [],
      "source": [
        "# Demo 3: Compare semantic vs. what keyword search would find\n",
        "print(\"üîç Search: 'space adventure'\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nüìä Semantic Search Results (finds movies by MEANING):\")\n",
        "results = semantic_search(\"space adventure\")\n",
        "display(results)\n",
        "\n",
        "# Now show what a simple keyword search would find\n",
        "print(\"\\nüìä Traditional Keyword Search (finds movies by EXACT WORDS):\")\n",
        "conn = get_connection(DB_NAME)\n",
        "keyword_results = pd.read_sql(\"\"\"\n",
        "    SELECT title, year, LEFT(summary, 100) || '...' as summary_preview\n",
        "    FROM movies\n",
        "    WHERE LOWER(title) LIKE '%space%'\n",
        "       OR LOWER(summary) LIKE '%space adventure%'\n",
        "    LIMIT 5;\n",
        "\"\"\", conn)\n",
        "conn.close()\n",
        "display(keyword_results)\n",
        "\n",
        "print(\"\\nüí° Notice how semantic search finds thematically similar movies\")\n",
        "print(\"   even if 'space adventure' doesn't appear in the text!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ060Z5AmPo8"
      },
      "source": [
        "---\n",
        "## Step 10: Verify Columnar Engine\n",
        "\n",
        "AlloyDB's columnar engine accelerates analytical queries by up to 100x. It works automatically‚ÄîAlloyDB identifies analytical query patterns and creates optimized columnar representations.\n",
        "\n",
        "Let's verify it's enabled on your instance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNmaX-yPmPo8"
      },
      "outputs": [],
      "source": [
        "# Check columnar engine settings\n",
        "conn = get_connection(DB_NAME)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "print(\"üîß Columnar Engine Configuration\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "cursor.execute(\"\"\"\n",
        "    SELECT name, setting, short_desc\n",
        "    FROM pg_settings\n",
        "    WHERE name LIKE '%columnar%' OR name LIKE '%google_columnar%'\n",
        "    ORDER BY name;\n",
        "\"\"\")\n",
        "\n",
        "results = cursor.fetchall()\n",
        "if results:\n",
        "    for name, setting, desc in results:\n",
        "        print(f\"   {name}: {setting}\")\n",
        "    print(\"\\n‚úÖ Columnar engine is configured!\")\n",
        "    print(\"   Analytical queries will be automatically accelerated.\")\n",
        "else:\n",
        "    print(\"   No columnar settings found (may be auto-configured)\")\n",
        "\n",
        "cursor.close()\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5U1wiAxmPo8"
      },
      "source": [
        "---\n",
        "## üéâ Congratulations!\n",
        "\n",
        "Your CymbalFlix database is fully operational! Here's what you've accomplished:\n",
        "\n",
        "### Database Setup\n",
        "- ‚úÖ Connected to AlloyDB using **IAM authentication** (no passwords!)\n",
        "- ‚úÖ Created a dedicated `cymbalflix` database\n",
        "- ‚úÖ Enabled vector, ScaNN, and ML integration extensions\n",
        "- ‚úÖ Registered Vertex AI model endpoints\n",
        "\n",
        "### Data Loading\n",
        "- ‚úÖ Bulk loaded ~9,700 movies with AI-generated summaries using **batch inserts**\n",
        "- ‚úÖ Added 3072-dimensional vector embeddings for semantic search\n",
        "- ‚úÖ Normalized genres into a proper relational structure\n",
        "- ‚úÖ Loaded 100,000+ ratings and 3,600+ tags at high speed\n",
        "- ‚úÖ Added external links (IMDb, TMDb)\n",
        "\n",
        "### AI Features\n",
        "- ‚úÖ Created a ScaNN index for lightning-fast vector similarity\n",
        "- ‚úÖ Tested semantic search that finds movies by meaning\n",
        "- ‚úÖ Verified columnar engine for analytical acceleration\n",
        "\n",
        "### Performance Highlight ‚ö°\n",
        "\n",
        "By using batch loading with `executemany()` instead of row-by-row inserts, we achieved:\n",
        "- **10-50x faster** data loading\n",
        "- Reduced round-trips to the database\n",
        "- Efficient batched transactions\n",
        "\n",
        "### Security Highlight üîê\n",
        "\n",
        "Notice how we never handled a database password? That's **IAM authentication** in action:\n",
        "- Your Google Cloud identity IS your database identity\n",
        "- The Python Connector handles secure token exchange automatically\n",
        "- No credentials to rotate, leak, or accidentally commit to Git\n",
        "\n",
        "This is the **production-ready** way to handle database authentication in Google Cloud.\n",
        "\n",
        "---\n",
        "\n",
        "### What's Next?\n",
        "\n",
        "Return to the lab instructions for **Task 4**, where you'll build the CymbalFlix Discover web application using Streamlit. You'll create a user interface that lets anyone search for movies semantically and explore AI-powered recommendations!\n",
        "\n",
        "üé¨ Your database is ready to power an AI-driven movie discovery experience! ü§ñ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nz_8uR9kmPo8"
      },
      "outputs": [],
      "source": [
        "# Cleanup: Close the connector when done\n",
        "# Uncomment the line below when you're finished with the notebook\n",
        "# connector.close()\n",
        "# print(\"‚úÖ Connector closed.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "name": "cymbalflix_database_setup.ipynb"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}